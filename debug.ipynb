{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before date conversion:\n",
      "              Datetime  ABI   ACT  AMA  ALB  JFK  LGA\n",
      "0  1949-01-01 00:00:00  6.1  10.6  2.2 -3.3 -3.3 -3.9\n",
      "1  1949-01-01 01:00:00  3.3   6.7  1.1 -3.3 -3.3 -3.9\n",
      "2  1949-01-01 02:00:00  2.2   5.6  0.6 -3.3 -3.3 -3.9\n",
      "3  1949-01-01 03:00:00  1.7   4.4  0.0 -2.8 -3.3 -3.9\n",
      "4  1949-01-01 04:00:00  1.7   3.9  1.1 -2.8 -3.3 -3.9\n",
      "After filtering for >=2010 and date conversion:\n",
      "                Datetime  ABI  ACT  AMA  ALB  JFK  LGA\n",
      "470398  2010-01-01 00:00  1.1  6.7 -0.6 -1.7  1.1  1.1\n",
      "470399  2010-01-01 01:00  1.1  6.1 -3.3 -1.7  1.1  1.1\n",
      "470400  2010-01-01 02:00  1.7  5.6 -3.9 -1.7  0.6  1.1\n",
      "470401  2010-01-01 03:00  1.1  5.0 -3.9 -1.7  0.6  1.1\n",
      "470402  2010-01-01 04:00  1.1  4.4 -5.0 -1.1  0.6  0.6\n",
      "Modified file saved as S:\\spatiotemporal-analysis\\spacetimeformer-main\\spacetimeformer\\data\\temperature-v1_modified.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the downloaded file in Kaggle environment\n",
    "file_path = \"S:\\\\spatiotemporal-analysis\\\\spacetimeformer-main\\\\spacetimeformer\\\\data\\\\temperature-v1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "print(\"Before date conversion:\")\n",
    "print(data.head())\n",
    "\n",
    "# Convert the 'Datetime' column to datetime\n",
    "data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "\n",
    "# Filter the data to keep only rows from the year 2010\n",
    "data = data[data['Datetime'].dt.year >= 2010]\n",
    "\n",
    "# Format the 'Datetime' column to the desired format: %Y-%m-%d %H:%M\n",
    "data['Datetime'] = data['Datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Display the first few rows after filtering and conversion\n",
    "print(\"After filtering for >=2010 and date conversion:\")\n",
    "print(data.head())\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "output_file_path = \"S:\\\\spatiotemporal-analysis\\\\spacetimeformer-main\\\\spacetimeformer\\\\data\\\\temperature-v1_modified.csv\"\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Confirm that the file has been saved\n",
    "print(f\"Modified file saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python S:\\\\spatiotemporal-analysis\\\\spacetimeformer-main\\\\spacetimeformer\\\\train.py spacetimeformer asos --context_points 128 --target_points 40 --run_name spacetimeformer_asos --batch_size 32 --warmup_steps 1000 --d_model 200 --d_ff 700 --enc_layers 1 --dec_layers 1 --dropout_emb .1 --dropout_ff .3 --d_qk 30 --d_v 30 --n_heads 10 --patience 10 --decay_factor .8 --data_path S:\\\\spatiotemporal-analysis\\\\spacetimeformer-main\\\\spacetimeformer\\\\data\\\\temperature-v1_modified.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def subset_csv(src_path: str, dest_path: str, columns: list):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from src_path, extracts the specified columns, and writes the subset to dest_path.\n",
    "    \n",
    "    Parameters:\n",
    "        src_path (str): Path to the source CSV file.\n",
    "        dest_path (str): Path to save the subset CSV file.\n",
    "        columns (list): List of column names to extract from the source CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(src_path)\n",
    "    \n",
    "    # Create a subset of the DataFrame with the specified columns\n",
    "    subset = df[columns]\n",
    "    \n",
    "    # Save the subset to the destination CSV file\n",
    "    subset.to_csv(dest_path, index=False)\n",
    "    print(f\"Subset saved to {dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset saved to /Users/sathwick/SDrive/ISU/S25/599/aemo_nsw_vlc.csv\n",
      "Subset saved to /Users/sathwick/SDrive/ISU/S25/599/aemo_nsw_qld.csv\n"
     ]
    }
   ],
   "source": [
    "subset_csv('/Users/sathwick/SDrive/ISU/S25/599/aemo_merged.csv', '/Users/sathwick/SDrive/ISU/S25/599/aemo_nsw_vlc.csv', ['SETTLEMENTDATE', \"TOTALDEMAND_NSW\", \"RRP_NSW\", \"TOTALDEMAND_VIC\", \"RRP_VIC\",])\n",
    "subset_csv('/Users/sathwick/SDrive/ISU/S25/599/aemo_merged.csv', '/Users/sathwick/SDrive/ISU/S25/599/aemo_nsw_qld.csv', ['SETTLEMENTDATE', \"TOTALDEMAND_NSW\", \"RRP_NSW\", \"TOTALDEMAND_QLD\", \"RRP_QLD\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SETTLEMENTDATE</th>\n",
       "      <th>TOTALDEMAND_NSW</th>\n",
       "      <th>RRP_NSW</th>\n",
       "      <th>TOTALDEMAND_VIC</th>\n",
       "      <th>RRP_VIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-01 00:00</td>\n",
       "      <td>6529.375000</td>\n",
       "      <td>19.210000</td>\n",
       "      <td>4485.130000</td>\n",
       "      <td>14.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-01 01:00</td>\n",
       "      <td>6145.710000</td>\n",
       "      <td>18.195000</td>\n",
       "      <td>4055.830000</td>\n",
       "      <td>12.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-01 02:00</td>\n",
       "      <td>5782.555000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>3701.360000</td>\n",
       "      <td>10.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-01 03:00</td>\n",
       "      <td>5600.055000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>3488.495000</td>\n",
       "      <td>6.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01 04:00</td>\n",
       "      <td>5619.520000</td>\n",
       "      <td>15.955000</td>\n",
       "      <td>3455.595000</td>\n",
       "      <td>4.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88100</th>\n",
       "      <td>2025-02-18 20:00</td>\n",
       "      <td>7977.682500</td>\n",
       "      <td>106.371667</td>\n",
       "      <td>5107.343333</td>\n",
       "      <td>93.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88101</th>\n",
       "      <td>2025-02-18 21:00</td>\n",
       "      <td>7653.483333</td>\n",
       "      <td>100.095833</td>\n",
       "      <td>4648.734167</td>\n",
       "      <td>87.070833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88102</th>\n",
       "      <td>2025-02-18 22:00</td>\n",
       "      <td>7361.791667</td>\n",
       "      <td>97.980000</td>\n",
       "      <td>4341.420833</td>\n",
       "      <td>84.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88103</th>\n",
       "      <td>2025-02-18 23:00</td>\n",
       "      <td>7002.186667</td>\n",
       "      <td>96.224167</td>\n",
       "      <td>4404.169167</td>\n",
       "      <td>86.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88104</th>\n",
       "      <td>2025-02-19 00:00</td>\n",
       "      <td>6896.590000</td>\n",
       "      <td>85.940000</td>\n",
       "      <td>4305.290000</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88105 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SETTLEMENTDATE  TOTALDEMAND_NSW     RRP_NSW  TOTALDEMAND_VIC  \\\n",
       "0      2015-02-01 00:00      6529.375000   19.210000      4485.130000   \n",
       "1      2015-02-01 01:00      6145.710000   18.195000      4055.830000   \n",
       "2      2015-02-01 02:00      5782.555000   16.450000      3701.360000   \n",
       "3      2015-02-01 03:00      5600.055000   14.200000      3488.495000   \n",
       "4      2015-02-01 04:00      5619.520000   15.955000      3455.595000   \n",
       "...                 ...              ...         ...              ...   \n",
       "88100  2025-02-18 20:00      7977.682500  106.371667      5107.343333   \n",
       "88101  2025-02-18 21:00      7653.483333  100.095833      4648.734167   \n",
       "88102  2025-02-18 22:00      7361.791667   97.980000      4341.420833   \n",
       "88103  2025-02-18 23:00      7002.186667   96.224167      4404.169167   \n",
       "88104  2025-02-19 00:00      6896.590000   85.940000      4305.290000   \n",
       "\n",
       "         RRP_VIC  \n",
       "0      14.590000  \n",
       "1      12.925000  \n",
       "2      10.440000  \n",
       "3       6.660000  \n",
       "4       4.380000  \n",
       "...          ...  \n",
       "88100  93.497500  \n",
       "88101  87.070833  \n",
       "88102  84.615000  \n",
       "88103  86.157500  \n",
       "88104  79.000000  \n",
       "\n",
       "[88105 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/Users/sathwick/SDrive/ISU/S25/599/aemo_nsw_vlc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SETTLEMENTDATE</th>\n",
       "      <th>TOTALDEMAND_NSW</th>\n",
       "      <th>RRP_NSW</th>\n",
       "      <th>TOTALDEMAND_QLD</th>\n",
       "      <th>RRP_QLD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-01 00:00</td>\n",
       "      <td>6529.375000</td>\n",
       "      <td>19.210000</td>\n",
       "      <td>5687.755000</td>\n",
       "      <td>17.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-01 01:00</td>\n",
       "      <td>6145.710000</td>\n",
       "      <td>18.195000</td>\n",
       "      <td>5349.120000</td>\n",
       "      <td>16.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-01 02:00</td>\n",
       "      <td>5782.555000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>5148.620000</td>\n",
       "      <td>14.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-01 03:00</td>\n",
       "      <td>5600.055000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>5045.510000</td>\n",
       "      <td>13.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01 04:00</td>\n",
       "      <td>5619.520000</td>\n",
       "      <td>15.955000</td>\n",
       "      <td>5066.075000</td>\n",
       "      <td>14.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88100</th>\n",
       "      <td>2025-02-18 20:00</td>\n",
       "      <td>7977.682500</td>\n",
       "      <td>106.371667</td>\n",
       "      <td>8026.143333</td>\n",
       "      <td>112.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88101</th>\n",
       "      <td>2025-02-18 21:00</td>\n",
       "      <td>7653.483333</td>\n",
       "      <td>100.095833</td>\n",
       "      <td>7663.199167</td>\n",
       "      <td>102.720833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88102</th>\n",
       "      <td>2025-02-18 22:00</td>\n",
       "      <td>7361.791667</td>\n",
       "      <td>97.980000</td>\n",
       "      <td>7187.470833</td>\n",
       "      <td>102.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88103</th>\n",
       "      <td>2025-02-18 23:00</td>\n",
       "      <td>7002.186667</td>\n",
       "      <td>96.224167</td>\n",
       "      <td>6736.655000</td>\n",
       "      <td>100.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88104</th>\n",
       "      <td>2025-02-19 00:00</td>\n",
       "      <td>6896.590000</td>\n",
       "      <td>85.940000</td>\n",
       "      <td>6531.180000</td>\n",
       "      <td>87.480000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88105 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SETTLEMENTDATE  TOTALDEMAND_NSW     RRP_NSW  TOTALDEMAND_QLD  \\\n",
       "0      2015-02-01 00:00      6529.375000   19.210000      5687.755000   \n",
       "1      2015-02-01 01:00      6145.710000   18.195000      5349.120000   \n",
       "2      2015-02-01 02:00      5782.555000   16.450000      5148.620000   \n",
       "3      2015-02-01 03:00      5600.055000   14.200000      5045.510000   \n",
       "4      2015-02-01 04:00      5619.520000   15.955000      5066.075000   \n",
       "...                 ...              ...         ...              ...   \n",
       "88100  2025-02-18 20:00      7977.682500  106.371667      8026.143333   \n",
       "88101  2025-02-18 21:00      7653.483333  100.095833      7663.199167   \n",
       "88102  2025-02-18 22:00      7361.791667   97.980000      7187.470833   \n",
       "88103  2025-02-18 23:00      7002.186667   96.224167      6736.655000   \n",
       "88104  2025-02-19 00:00      6896.590000   85.940000      6531.180000   \n",
       "\n",
       "          RRP_QLD  \n",
       "0       17.855000  \n",
       "1       16.100000  \n",
       "2       14.420000  \n",
       "3       13.025000  \n",
       "4       14.620000  \n",
       "...           ...  \n",
       "88100  112.365000  \n",
       "88101  102.720833  \n",
       "88102  102.015000  \n",
       "88103  100.227500  \n",
       "88104   87.480000  \n",
       "\n",
       "[88105 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/Users/sathwick/SDrive/ISU/S25/599/aemo_nsw_qld.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_columns_and_save(input_file_path, selected_columns, output_file_path):\n",
    "    # Read the original dataframe from the input CSV file\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Filter the dataframe to only include the selected columns\n",
    "    df_filtered = df[selected_columns]\n",
    "    \n",
    "    # Save the filtered dataframe to the specified output file path\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "    print(f\"Filtered dataframe saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\TAS.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\QLD.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\SA.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_QLD.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\QLD_SA.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\TAS_VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_SA_VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_QLD_VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_QLD_SA.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_TAS_VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_QLD_SA_VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_SA_TAS_VIC.csv\n",
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\NSW_QLD_SA_TAS_VIC.csv\n"
     ]
    }
   ],
   "source": [
    "l = [['NSW'],\n",
    " ['TAS'],\n",
    " ['QLD'],\n",
    " ['VIC'],\n",
    " ['SA'],\n",
    " ['NSW', 'QLD'],\n",
    " ['NSW', 'VIC'],\n",
    " ['QLD', 'SA'],\n",
    " ['TAS', 'VIC'],\n",
    " ['NSW', 'SA', 'VIC'],\n",
    " ['NSW', 'QLD', 'VIC'],\n",
    " ['NSW', 'QLD', 'SA'],\n",
    " ['NSW', 'TAS', 'VIC'],\n",
    " ['NSW', 'QLD', 'SA', 'VIC'],\n",
    " ['NSW', 'SA', 'TAS', 'VIC'],\n",
    " ['NSW', 'QLD', 'SA', 'TAS', 'VIC']]\n",
    "\n",
    "for i in l:\n",
    "    input_file_path = 'C:\\\\Users\\\\vm-user\\\\Downloads\\\\aemo_merged.csv'\n",
    "    selected_columns = ['SETTLEMENTDATE']\n",
    "    for j in i:\n",
    "        selected_columns.append(f'TOTALDEMAND_{j}')\n",
    "        selected_columns.append(f'RRP_{j}')\n",
    "    output_file_path = f'C:\\\\Users\\\\vm-user\\\\Downloads\\\\{\"_\".join(i)}.csv'\n",
    "    filter_columns_and_save(input_file_path, selected_columns, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\aemo_nsw_qld_vic.csv\n"
     ]
    }
   ],
   "source": [
    "filter_columns_and_save(\n",
    "    \"C:\\\\Users\\\\vm-user\\\\Downloads\\\\aemo_merged.csv\", \n",
    "    [\"SETTLEMENTDATE\", \"TOTALDEMAND_NSW\", \"RRP_NSW\", \"TOTALDEMAND_QLD\", \"RRP_QLD\", \"TOTALDEMAND_VIC\", \"RRP_VIC\"], \n",
    "    \"C:\\\\Users\\\\vm-user\\\\Downloads\\\\aemo_nsw_qld_vic.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe saved to C:\\Users\\vm-user\\Downloads\\aemo_nsw_qld_vic_tas.csv\n"
     ]
    }
   ],
   "source": [
    "filter_columns_and_save(\n",
    "    \"C:\\\\Users\\\\vm-user\\\\Downloads\\\\aemo_merged.csv\", \n",
    "    [\"SETTLEMENTDATE\", \"TOTALDEMAND_NSW\", \"RRP_NSW\", \"TOTALDEMAND_QLD\", \"RRP_QLD\", \"TOTALDEMAND_VIC\", \"RRP_VIC\", \"TOTALDEMAND_TAS\", \"RRP_TAS\"], \n",
    "    \"C:\\\\Users\\\\vm-user\\\\Downloads\\\\aemo_nsw_qld_vic_tas.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = pd.read_csv(\"C:\\\\Users\\\\vm-user\\\\Downloads\\\\all_countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>ISO3 Code</th>\n",
       "      <th>Datetime (UTC)</th>\n",
       "      <th>Datetime (Local)</th>\n",
       "      <th>Price (EUR/MWhe)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>22.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>BEL</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>36.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Czechia</td>\n",
       "      <td>CZE</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>DNK</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>18.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>EST</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>23.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country ISO3 Code       Datetime (UTC)     Datetime (Local)  \\\n",
       "0  Austria       AUT  2015-01-01 00:00:00  2015-01-01 01:00:00   \n",
       "1  Belgium       BEL  2015-01-01 00:00:00  2015-01-01 01:00:00   \n",
       "2  Czechia       CZE  2015-01-01 00:00:00  2015-01-01 01:00:00   \n",
       "3  Denmark       DNK  2015-01-01 00:00:00  2015-01-01 01:00:00   \n",
       "4  Estonia       EST  2015-01-01 00:00:00  2015-01-01 02:00:00   \n",
       "\n",
       "   Price (EUR/MWhe)  \n",
       "0             22.34  \n",
       "1             36.56  \n",
       "2             24.20  \n",
       "3             18.29  \n",
       "4             23.37  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_map = {\n",
    "    'Austria': 'AT',\n",
    "    'Belgium': 'BE',\n",
    "    'Bulgaria': 'BG',\n",
    "    'Croatia': 'HR',\n",
    "    'Czechia': 'CZ',\n",
    "    'Denmark': 'DK',\n",
    "    'Estonia': 'EE',\n",
    "    'Finland': 'FI',\n",
    "    'France': 'FR',\n",
    "    'Germany': 'DE',\n",
    "    'Greece': 'GR',\n",
    "    'Hungary': 'HU',\n",
    "    'Ireland': 'IE',\n",
    "    'Italy': 'IT',\n",
    "    'Latvia': 'LV',\n",
    "    'Lithuania': 'LT',\n",
    "    'Luxembourg': 'LU',\n",
    "    'Montenegro': 'ME',\n",
    "    'Netherlands': 'NL',\n",
    "    'North Macedonia': 'MK',\n",
    "    'Norway': 'NO',\n",
    "    'Poland': 'PL',\n",
    "    'Portugal': 'PT',\n",
    "    'Romania': 'RO',\n",
    "    'Serbia': 'RS',\n",
    "    'Slovakia': 'SK',\n",
    "    'Slovenia': 'SI',\n",
    "    'Spain': 'ES',\n",
    "    'Sweden': 'SE',\n",
    "    'Switzerland': 'CH',\n",
    "    'United Kingdom': 'GB'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 31)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(countries_map.values())), len(countries_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique two-letter country codes in Country column: ['AT' 'BE' 'CZ' 'DK' 'EE' 'FI' 'FR' 'DE' 'GR' 'HU' 'IT' 'LV' 'LT' 'LU'\n",
      " 'NL' 'NO' 'PL' 'PT' 'RO' 'SK' 'SI' 'ES' 'SE' 'CH' 'GB' 'BG' 'RS' 'HR'\n",
      " 'ME' 'MK' 'IE']\n"
     ]
    }
   ],
   "source": [
    "# Replace the Country column values with their two-letter codes\n",
    "all_countries['Country'] = all_countries['Country'].map(countries_map)\n",
    "\n",
    "# Optionally, check the result\n",
    "print(\"Unique two-letter country codes in Country column:\", all_countries['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 31)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(countries_map.values())), len(countries_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def pivot_and_resample(df):\n",
    "    \"\"\"\n",
    "    Transforms the dataset by pivoting countries into columns and resampling the time series.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe containing country-wise electricity prices.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Resampled dataframe with hourly prices for each country as separate columns.\n",
    "    \"\"\"\n",
    "    # Convert 'Datetime (UTC)' to datetime format\n",
    "    df['Datetime (UTC)'] = pd.to_datetime(df['Datetime (UTC)'])\n",
    "\n",
    "    # Drop 'Datetime (Local)' as it's not needed\n",
    "    df = df.drop(columns=['Datetime (Local)'])\n",
    "\n",
    "    # Rename 'Price (EUR/MWhe)' to 'Price'\n",
    "    df = df.rename(columns={'Price (EUR/MWhe)': 'Price'})\n",
    "\n",
    "    # Pivot the table to make countries as columns\n",
    "    df_pivoted = df.pivot_table(index='Datetime (UTC)', columns='Country', values='Price', aggfunc='mean')\n",
    "\n",
    "    # Resample to ensure hourly intervals from min to max timestamp\n",
    "    df_resampled = df_pivoted.resample('h').mean()\n",
    "\n",
    "    # Reset index to bring datetime back as a column\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "# Example usage\n",
    "final_df = pivot_and_resample(all_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df.copy()\n",
    "for col in df.columns[1:]:\n",
    "    non_null_series = df[col].dropna()  # Remove NaN values to find the first non-null value\n",
    "    if not non_null_series.empty:\n",
    "        first_index = non_null_series.index[0]  # Find the first non-null value's index\n",
    "        first_value = non_null_series.iloc[0]  # Get the first non-null value\n",
    "        \n",
    "        # Replace consecutive occurrences with NaN, starting from first_index\n",
    "        for i in range(first_index + 1, len(df)):\n",
    "            if pd.isna(df[col].iloc[i]) or df[col].iloc[i] != first_value:\n",
    "                break  # Stop if a different value or NaN appears\n",
    "            df.loc[i-1, col] = float(\"nan\")  # Set previous consecutive occurrences to NaN\n",
    "\n",
    "# Remove initial rows where all country columns are NaN (excluding 'Datetime')\n",
    "df_cleaned = df.dropna(subset=df.columns[1:], how='all').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_1hr_intervals(time_series):\n",
    "    time_series = pd.to_datetime(time_series).sort_values()\n",
    "    expected_interval = pd.Timedelta(hours=1)\n",
    "    return (time_series.diff().dropna() == expected_interval).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_1hr_intervals(df_cleaned['Datetime (UTC)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Country</th>\n",
       "      <th>Datetime (UTC)</th>\n",
       "      <th>AT</th>\n",
       "      <th>BE</th>\n",
       "      <th>BG</th>\n",
       "      <th>CH</th>\n",
       "      <th>CZ</th>\n",
       "      <th>DE</th>\n",
       "      <th>DK</th>\n",
       "      <th>EE</th>\n",
       "      <th>ES</th>\n",
       "      <th>...</th>\n",
       "      <th>MK</th>\n",
       "      <th>NL</th>\n",
       "      <th>NO</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>RS</th>\n",
       "      <th>SE</th>\n",
       "      <th>SI</th>\n",
       "      <th>SK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.43</td>\n",
       "      <td>24.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.29</td>\n",
       "      <td>23.37</td>\n",
       "      <td>48.10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.37</td>\n",
       "      <td>23.25</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.08</td>\n",
       "      <td>22.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.04</td>\n",
       "      <td>19.33</td>\n",
       "      <td>47.33</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.33</td>\n",
       "      <td>22.20</td>\n",
       "      <td>22.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.47</td>\n",
       "      <td>20.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.60</td>\n",
       "      <td>17.66</td>\n",
       "      <td>42.27</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.66</td>\n",
       "      <td>19.56</td>\n",
       "      <td>20.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.83</td>\n",
       "      <td>19.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.95</td>\n",
       "      <td>17.53</td>\n",
       "      <td>38.41</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.53</td>\n",
       "      <td>18.88</td>\n",
       "      <td>19.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.26</td>\n",
       "      <td>17.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.50</td>\n",
       "      <td>18.07</td>\n",
       "      <td>35.72</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.07</td>\n",
       "      <td>18.39</td>\n",
       "      <td>17.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88818</th>\n",
       "      <td>2025-02-17 18:00:00</td>\n",
       "      <td>214.41</td>\n",
       "      <td>215.46</td>\n",
       "      <td>224.35</td>\n",
       "      <td>203.38</td>\n",
       "      <td>216.50</td>\n",
       "      <td>214.10</td>\n",
       "      <td>215.38</td>\n",
       "      <td>219.35</td>\n",
       "      <td>215.52</td>\n",
       "      <td>...</td>\n",
       "      <td>148.2</td>\n",
       "      <td>215.43</td>\n",
       "      <td>94.09</td>\n",
       "      <td>218.20</td>\n",
       "      <td>215.52</td>\n",
       "      <td>216.86</td>\n",
       "      <td>223.78</td>\n",
       "      <td>157.34</td>\n",
       "      <td>216.46</td>\n",
       "      <td>217.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88819</th>\n",
       "      <td>2025-02-17 19:00:00</td>\n",
       "      <td>178.47</td>\n",
       "      <td>177.95</td>\n",
       "      <td>224.35</td>\n",
       "      <td>171.35</td>\n",
       "      <td>179.08</td>\n",
       "      <td>181.78</td>\n",
       "      <td>177.87</td>\n",
       "      <td>180.91</td>\n",
       "      <td>200.00</td>\n",
       "      <td>...</td>\n",
       "      <td>148.2</td>\n",
       "      <td>177.93</td>\n",
       "      <td>88.03</td>\n",
       "      <td>180.90</td>\n",
       "      <td>200.00</td>\n",
       "      <td>179.47</td>\n",
       "      <td>195.21</td>\n",
       "      <td>130.24</td>\n",
       "      <td>179.03</td>\n",
       "      <td>179.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88820</th>\n",
       "      <td>2025-02-17 20:00:00</td>\n",
       "      <td>159.23</td>\n",
       "      <td>155.38</td>\n",
       "      <td>224.35</td>\n",
       "      <td>159.98</td>\n",
       "      <td>151.40</td>\n",
       "      <td>161.50</td>\n",
       "      <td>155.19</td>\n",
       "      <td>155.95</td>\n",
       "      <td>155.62</td>\n",
       "      <td>...</td>\n",
       "      <td>148.2</td>\n",
       "      <td>155.30</td>\n",
       "      <td>82.59</td>\n",
       "      <td>154.09</td>\n",
       "      <td>155.62</td>\n",
       "      <td>165.32</td>\n",
       "      <td>174.31</td>\n",
       "      <td>110.05</td>\n",
       "      <td>161.59</td>\n",
       "      <td>172.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88821</th>\n",
       "      <td>2025-02-17 21:00:00</td>\n",
       "      <td>159.23</td>\n",
       "      <td>139.51</td>\n",
       "      <td>224.35</td>\n",
       "      <td>159.98</td>\n",
       "      <td>151.40</td>\n",
       "      <td>161.50</td>\n",
       "      <td>155.19</td>\n",
       "      <td>155.95</td>\n",
       "      <td>140.00</td>\n",
       "      <td>...</td>\n",
       "      <td>148.2</td>\n",
       "      <td>139.39</td>\n",
       "      <td>82.59</td>\n",
       "      <td>154.09</td>\n",
       "      <td>155.62</td>\n",
       "      <td>161.88</td>\n",
       "      <td>174.31</td>\n",
       "      <td>110.05</td>\n",
       "      <td>161.59</td>\n",
       "      <td>172.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88822</th>\n",
       "      <td>2025-02-17 22:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88823 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Country      Datetime (UTC)      AT      BE      BG      CH      CZ      DE  \\\n",
       "0       2015-01-01 00:00:00     NaN     NaN     NaN   43.43   24.20     NaN   \n",
       "1       2015-01-01 01:00:00     NaN     NaN     NaN   38.08   22.06     NaN   \n",
       "2       2015-01-01 02:00:00     NaN     NaN     NaN   35.47   20.27     NaN   \n",
       "3       2015-01-01 03:00:00     NaN     NaN     NaN   30.83   19.17     NaN   \n",
       "4       2015-01-01 04:00:00     NaN     NaN     NaN   28.26   17.90     NaN   \n",
       "...                     ...     ...     ...     ...     ...     ...     ...   \n",
       "88818   2025-02-17 18:00:00  214.41  215.46  224.35  203.38  216.50  214.10   \n",
       "88819   2025-02-17 19:00:00  178.47  177.95  224.35  171.35  179.08  181.78   \n",
       "88820   2025-02-17 20:00:00  159.23  155.38  224.35  159.98  151.40  161.50   \n",
       "88821   2025-02-17 21:00:00  159.23  139.51  224.35  159.98  151.40  161.50   \n",
       "88822   2025-02-17 22:00:00     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "Country      DK      EE      ES  ...     MK      NL     NO      PL      PT  \\\n",
       "0         18.29   23.37   48.10  ...    NaN     NaN  27.35     NaN   48.10   \n",
       "1         16.04   19.33   47.33  ...    NaN     NaN  27.23     NaN   47.33   \n",
       "2         14.60   17.66   42.27  ...    NaN     NaN  27.15     NaN   42.27   \n",
       "3         14.95   17.53   38.41  ...    NaN     NaN  27.14     NaN   38.41   \n",
       "4         14.50   18.07   35.72  ...    NaN     NaN  27.29     NaN   35.72   \n",
       "...         ...     ...     ...  ...    ...     ...    ...     ...     ...   \n",
       "88818    215.38  219.35  215.52  ...  148.2  215.43  94.09  218.20  215.52   \n",
       "88819    177.87  180.91  200.00  ...  148.2  177.93  88.03  180.90  200.00   \n",
       "88820    155.19  155.95  155.62  ...  148.2  155.30  82.59  154.09  155.62   \n",
       "88821    155.19  155.95  140.00  ...  148.2  139.39  82.59  154.09  155.62   \n",
       "88822       NaN     NaN     NaN  ...    NaN     NaN    NaN     NaN     NaN   \n",
       "\n",
       "Country      RO      RS      SE      SI      SK  \n",
       "0           NaN     NaN   23.37   23.25   24.20  \n",
       "1           NaN     NaN   19.33   22.20   22.06  \n",
       "2           NaN     NaN   17.66   19.56   20.27  \n",
       "3           NaN     NaN   17.53   18.88   19.17  \n",
       "4           NaN     NaN   18.07   18.39   17.90  \n",
       "...         ...     ...     ...     ...     ...  \n",
       "88818    216.86  223.78  157.34  216.46  217.09  \n",
       "88819    179.47  195.21  130.24  179.03  179.71  \n",
       "88820    165.32  174.31  110.05  161.59  172.18  \n",
       "88821    161.88  174.31  110.05  161.59  172.18  \n",
       "88822       NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[88823 rows x 32 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_clean_dataframe(df, selected_columns):\n",
    "    \"\"\"\n",
    "    Filters the given DataFrame to retain only the selected columns, \n",
    "    and removes initial and bottom-most rows where all selected columns are NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        selected_columns (list): List of column names to retain.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure 'Datetime (UTC)' is included in the filtered columns\n",
    "    filtered_columns = ['Datetime (UTC)'] + selected_columns if 'Datetime (UTC)' in df.columns else selected_columns\n",
    "    df_filtered = df[filtered_columns]\n",
    "\n",
    "    # Drop initial rows where all selected columns (excluding 'Datetime') are NaN\n",
    "    start_index = df_filtered[selected_columns].first_valid_index()\n",
    "    df_cleaned = df_filtered.loc[start_index:].reset_index(drop=True)\n",
    "    \n",
    "    # Drop bottom-most rows where all selected columns (excluding 'Datetime') are NaN\n",
    "    end_index = df_cleaned[selected_columns].last_valid_index()\n",
    "    df_cleaned = df_cleaned.loc[:end_index].reset_index(drop=True)\n",
    "    df_cleaned.fillna(10**6, inplace=True)  # Replace NaNs with a large number\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# List of countries\n",
    "countries = ['Austria', 'Belgium', 'Czechia', 'Denmark', 'Estonia', 'Finland',\n",
    "'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Latvia',\n",
    "'Lithuania', 'Luxembourg', 'Netherlands', 'Norway', 'Poland',\n",
    "'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden',\n",
    "'Switzerland', 'United Kingdom', 'Bulgaria', 'Serbia', 'Croatia',\n",
    "'Montenegro', 'North Macedonia', 'Ireland']\n",
    "\n",
    "lat = [47.516231, 50.503887, 49.817492, 56.26392, 58.595272, 61.92411,\n",
    "       46.227638, 51.165691, 39.074208, 47.162494, 41.87194, 56.879635,\n",
    "       55.169438, 49.815273, 52.132633, 60.472024, 51.919438, 39.399872,\n",
    "       45.943161, 48.669026, 46.151241, 40.463667, 60.128161, 46.818188,\n",
    "       55.378051, 42.733883, 44.016521, 45.1, 42.708678, 41.608635, 53.41291]\n",
    "\n",
    "long = [14.550072, 4.469936, 15.472962, 9.501785, 25.013607, 25.748151,\n",
    "        2.213749, 10.451526, 21.824312, 19.503304, 12.56738, 24.603189,\n",
    "        23.881275, 6.129583, 5.291266, 8.468946, 19.145136, -8.224454,\n",
    "        24.96676, 19.699024, 14.995463, -3.74922, 18.643501, 8.227512,\n",
    "        -3.435973, 25.48583, 21.005859, 15.2, 19.37439, 21.745275, -8.24389]\n",
    "\n",
    "area_km_sq = [82520, 30280, 77187, 42434, 42388, 303815, \n",
    "       640427, 349390, 128900, 91260, 295717, 62230,\n",
    "       62610, 2574, 33893, 366704, 304255,\n",
    "       91606, 230080, 48080, 20151, 498980, 407284,\n",
    "       39510, 242741, 108489, 88499, 55974,\n",
    "       13452, 25220, 68883]\n",
    "\n",
    "eu_data = {\n",
    "    'countries': countries,\n",
    "    'lat': lat,\n",
    "    'long': long,\n",
    "    'area_km_sq': area_km_sq\n",
    "}\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "    \n",
    "    # Differences in coordinates\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Function to calculate the effective radius based on area\n",
    "def effective_radius(area):\n",
    "    return np.sqrt(area / np.pi)\n",
    "\n",
    "# Prepare the data into a DataFrame\n",
    "eu_df = pd.DataFrame(eu_data)\n",
    "\n",
    "# Calculate the effective radius for each country\n",
    "eu_df['effective_radius'] = eu_df['area_km_sq'].apply(effective_radius)\n",
    "\n",
    "# Initialize list to store both normalized and physical distances\n",
    "distances = []\n",
    "\n",
    "# Loop through pairs of countries to calculate distances\n",
    "for i in range(len(eu_df)):\n",
    "    for j in range(len(eu_df)):\n",
    "        # Get the coordinates, areas and radii\n",
    "        lat1, lon1, r1 = eu_df.iloc[i]['lat'], eu_df.iloc[i]['long'], eu_df.iloc[i]['effective_radius']\n",
    "        lat2, lon2, r2 = eu_df.iloc[j]['lat'], eu_df.iloc[j]['long'], eu_df.iloc[j]['effective_radius']\n",
    "        \n",
    "        # Calculate the physical distance between the centroids using the Haversine formula\n",
    "        distance = haversine(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "        # Calculate the normalized distance\n",
    "        D_norm = distance / (r1 + r2)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        distances.append({\n",
    "            'Country 1': eu_df.iloc[i]['countries'],\n",
    "            'Country 2': eu_df.iloc[j]['countries'],\n",
    "            'Physical Distance (km)': distance,\n",
    "            'Normalized Distance': D_norm\n",
    "        })\n",
    "\n",
    "distances_df = pd.DataFrame(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country 1</th>\n",
       "      <th>Country 2</th>\n",
       "      <th>Physical Distance (km)</th>\n",
       "      <th>Normalized Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Austria</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>805.904977</td>\n",
       "      <td>3.096702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Czechia</td>\n",
       "      <td>264.705969</td>\n",
       "      <td>0.830276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>1031.865905</td>\n",
       "      <td>3.707866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Estonia</td>\n",
       "      <td>1412.866158</td>\n",
       "      <td>5.078086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>Serbia</td>\n",
       "      <td>2364.256758</td>\n",
       "      <td>7.483855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1923.646715</td>\n",
       "      <td>6.832214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>Montenegro</td>\n",
       "      <td>2350.806279</td>\n",
       "      <td>11.010233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>North Macedonia</td>\n",
       "      <td>2578.886347</td>\n",
       "      <td>10.850582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>961 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country 1        Country 2  Physical Distance (km)  Normalized Distance\n",
       "0     Austria          Austria                0.000000             0.000000\n",
       "1     Austria          Belgium              805.904977             3.096702\n",
       "2     Austria          Czechia              264.705969             0.830276\n",
       "3     Austria          Denmark             1031.865905             3.707866\n",
       "4     Austria          Estonia             1412.866158             5.078086\n",
       "..        ...              ...                     ...                  ...\n",
       "956   Ireland           Serbia             2364.256758             7.483855\n",
       "957   Ireland          Croatia             1923.646715             6.832214\n",
       "958   Ireland       Montenegro             2350.806279            11.010233\n",
       "959   Ireland  North Macedonia             2578.886347            10.850582\n",
       "960   Ireland          Ireland                0.000000             0.000000\n",
       "\n",
       "[961 rows x 4 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def closest_locations(df, location_col, other_location_col, distance_col, n):\n",
    "    # Create an empty dictionary to store the results\n",
    "    result = {}\n",
    "    \n",
    "    # Get unique values in the location column\n",
    "    unique_locations = df[location_col].unique()\n",
    "    \n",
    "    for location in unique_locations:\n",
    "        # Filter the dataframe for rows corresponding to the current location\n",
    "        location_df = df[df[location_col] == location]\n",
    "        \n",
    "        # Sort the rows based on the distance column\n",
    "        location_df = location_df.sort_values(by=distance_col)\n",
    "        \n",
    "        # Create subsets that progressively add the next 'n' closest locations\n",
    "        closest_n_locations = []\n",
    "            \n",
    "        num_steps = math.ceil(len(location_df) / n)\n",
    "        for i in range(1, num_steps + 1):\n",
    "            # Ensure we don't go past the length of the DataFrame\n",
    "            end_index = min(i * n, len(location_df))\n",
    "            closest_n_locations.append(location_df.iloc[:end_index][other_location_col].tolist())        # Add the result for the current location to the dictionary\n",
    "            result[location] = closest_n_locations\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distances_df[distances_df['Country 1'] == 'Austria'].sort_values(by='Normalized Distance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_subsets(closest_dict):\n",
    "    # Set to hold unique subsets (as sorted tuples)\n",
    "    unique_sets = set()\n",
    "\n",
    "    # Iterate over each location's list of subsets\n",
    "    for location, subsets in closest_dict.items():\n",
    "        for subset in subsets:\n",
    "            # Convert each subset to a sorted tuple (for order-insensitivity)\n",
    "            unique_sets.add(tuple(sorted(subset)))\n",
    "    \n",
    "    # Convert the set of unique tuples back to a list of lists\n",
    "    unique_subsets_list = [list(subset) for subset in unique_sets]\n",
    "    \n",
    "    # Sort the subsets by length\n",
    "    unique_subsets_list.sort(key=len)\n",
    "    \n",
    "    return unique_subsets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_subsets = unique_subsets(closest_locations(distances_df, 'Country 1', 'Country 2', 'Normalized Distance', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(country_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BG.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_HR.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_CZ.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_DK.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_EE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_FI.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_FR.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_DE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_GR.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_HU.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_IE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_IT.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_LV.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_LT.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_LU.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_ME.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_NL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_MK.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_NO.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_PL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_PT.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_RO.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_RS.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_SK.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_SI.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_ES.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_HR_CZ_FR_DE_HU_IT_PL_RS_SI.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_CZ_DK_FI_FR_DE_NL_NO_PL_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BG_GR_HU_IT_ME_MK_PL_RO_RS_SK.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BE_FR_DE_IT_LU_NL_PL_ES_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_CZ_EE_FI_DE_LV_LT_NO_PL_RO_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_CZ_DE_HU_LV_LT_PL_RO_SK_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_DK_EE_FI_DE_LV_LT_NO_PL_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BE_FR_DE_LU_NL_NO_PL_ES_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_HR_CZ_DE_HU_IT_PL_RO_RS_SK.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BE_DK_FR_DE_IE_NL_NO_ES_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BG_HR_GR_HU_IT_ME_MK_PL_RO_RS.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_HR_FR_DE_GR_IT_ME_RS_SI_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_CZ_DK_FR_DE_LU_NL_PL_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_FR_DE_IE_IT_NO_PL_PT_ES_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_HR_CZ_DE_HU_PL_RO_RS_SK_SI.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BE_DK_FR_DE_LU_NL_NO_PL_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_HR_CZ_FR_DE_HU_PL_RO_SK_SI.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BG_HR_GR_HU_IT_ME_MK_RO_RS_SK.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BE_FR_DE_IE_NL_NO_PL_ES_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_HR_CZ_FR_DE_HU_IT_PL_SK_SI.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_DK_EE_FI_FR_DE_LV_LT_NO_PL_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_GR_HU_ME_MK_PL_RO_RS_SK.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BE_FR_DE_IE_IT_NL_PT_ES_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_FR_DE_IT_PL_ES_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_DK_EE_FI_FR_DE_LV_NO_PL_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_EE_FI_FR_DE_LV_LT_NO_PL_RO_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_FR_DE_IT_LU_NL_ES_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_BG_HR_FR_GR_HU_IT_ME_MK_RO_RS.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_CZ_DK_EE_FI_FR_DE_HU_IT_LV_LT_NO_PL_RO_RS_SK_ES_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_FR_DE_HU_IT_LU_NL_NO_PL_RO_RS_SK_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_FI_FR_DE_GR_HU_IT_ME_MK_NO_PL_RO_RS_SK_SI_ES_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_DK_EE_FI_FR_DE_HU_IT_LV_LT_NO_PL_RO_RS_SK_SI_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_CZ_DK_EE_FI_FR_DE_HU_IT_LV_LT_NL_NO_PL_RO_RS_SK_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_CZ_DK_FI_FR_DE_HU_IE_IT_LU_NL_NO_PL_PT_RO_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_FR_DE_GR_HU_IT_LU_ME_MK_PL_RO_RS_SK_SI_ES_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_DK_FI_FR_DE_HU_IT_LT_NL_NO_PL_RO_RS_SK_SI_SE_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_DK_EE_FI_FR_DE_HU_IT_LV_LT_NO_PL_RO_RS_SK_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_DK_FR_DE_HU_IT_LT_LU_NL_NO_PL_RO_SK_SI_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_FI_FR_DE_GR_HU_IT_LV_LT_ME_MK_PL_RO_RS_SK_SI_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_FI_FR_DE_GR_HU_IT_LT_ME_MK_PL_RO_RS_SK_SI_ES_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_FR_DE_GR_HU_IT_LT_ME_MK_NO_PL_RO_RS_SK_SI_SE_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_CZ_DK_EE_FI_FR_DE_HU_IE_LV_LT_NL_NO_PL_RO_SK_ES_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_CZ_DK_EE_FI_FR_DE_HU_IE_IT_LV_LT_NL_NO_PL_RO_ES_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_FR_DE_GR_HU_IT_ME_NO_PL_RO_RS_SK_SI_ES_SE_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_CZ_DK_EE_FI_FR_DE_HU_IT_LV_LT_NL_NO_PL_RO_RS_SK_SE_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_FR_DE_GR_HU_IT_ME_MK_NO_PL_RO_RS_SK_SI_ES_SE_CH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BG_HR_CZ_FI_FR_DE_GR_HU_IT_LV_LT_ME_NO_PL_RO_RS_SK_SI_SE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_DK_FI_FR_DE_HU_IE_IT_LU_NL_NO_PL_RO_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_DK_FR_DE_HU_IE_IT_LU_NL_NO_PL_PT_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_FR_DE_HU_IE_IT_LU_NL_NO_PL_PT_RO_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_HR_CZ_FI_FR_DE_GR_HU_IE_IT_NL_NO_PL_PT_RO_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_DK_EE_FI_FR_DE_GR_HU_IT_LV_LT_LU_ME_NL_MK_NO_PL_PT_RO_RS_SK_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_DK_EE_FI_FR_DE_GR_HU_IE_IT_LV_LT_ME_NL_MK_NO_PL_PT_RO_RS_SK_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_DK_EE_FI_FR_DE_GR_HU_IE_IT_LV_LT_LU_ME_NL_NO_PL_PT_RO_RS_SK_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_DK_EE_FI_FR_DE_GR_HU_IE_IT_LV_LT_LU_NL_MK_NO_PL_PT_RO_RS_SK_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_DK_FI_FR_DE_GR_HU_IE_IT_LV_LT_LU_ME_NL_MK_NO_PL_PT_RO_RS_SK_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_DK_EE_FI_FR_DE_GR_HU_IE_IT_LV_LT_LU_ME_NL_MK_NO_PL_RO_RS_SK_SI_ES_SE_CH_GB.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\eu_AT_BE_BG_HR_CZ_DK_EE_FI_FR_DE_GR_HU_IE_IT_LV_LT_LU_ME_NL_MK_NO_PL_PT_RO_RS_SK_SI_ES_SE_CH_GB.csv\n"
     ]
    }
   ],
   "source": [
    "df_cleaned['Datetime (UTC)'] = pd.to_datetime(df_cleaned['Datetime (UTC)'])\n",
    "df_cleaned['Datetime (UTC)'] = df_cleaned['Datetime (UTC)'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "for country in countries_map:\n",
    "    df_to_save = filter_and_clean_dataframe(df_cleaned, [countries_map[country]])\n",
    "    output_file_path = f\"C:\\\\Users\\\\vm-user\\\\Downloads\\\\eu_{countries_map[country]}.csv\"\n",
    "    # Print a message to see if the df has null values\n",
    "    if df_to_save.isnull().values.any():\n",
    "        print(f\"Warning: DataFrame for {countries_map[country]} contains null values.\")\n",
    "    df_to_save.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved: {output_file_path}\")\n",
    "\n",
    "for subset in country_subsets:\n",
    "    subset = [countries_map[country] for country in subset]\n",
    "    df_to_save = filter_and_clean_dataframe(df_cleaned, subset)\n",
    "    if df_to_save.isnull().values.any():\n",
    "        print(f\"Warning: DataFrame for {countries_map[country]} contains null values.\")\n",
    "    output_file_path = f\"C:\\\\Users\\\\vm-user\\\\Downloads\\\\eu_{'_'.join(subset)}.csv\"\n",
    "    df_to_save.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 31)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(country_subsets[-1]) & set(country_subsets[-2])), len(set(country_subsets[-1]) | set(country_subsets[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(country_subsets[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyiso_data_df = pd.read_csv(\"C:\\\\Users\\\\vm-user\\\\Downloads\\\\nyiso.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "\n",
    "# List of countries\n",
    "regions = ['CAPITL', 'CENTRL', 'MHK VL', 'HUD VL', 'LONGIL', 'N.Y.C.', 'WEST', 'GENESE', 'NORTH', 'DUNWOD', 'MILLWD']\n",
    "\n",
    "long = [-73.9820406, -76.7300675, -75.1443401, -74.0753086, -73.0188755, -73.9247654, -78.6001935, -77.5107558, -74.0169224, -73.7846149, -73.8141997]\n",
    "\n",
    "lat = [43.1823555, 42.6094099, 43.4161459, 41.7112736, 40.8443418, 40.6944843, 42.6613719, 42.8694038, 44.5393904, 41.0350397, 41.2394963]\n",
    "\n",
    "area_km_sq = [41206.36, 39600.998, 74038.96, 18453.5, 5795.6, 1566.34, 29527.06, 9535.63, 15199.63, 905.66, 743.044]\n",
    "\n",
    "nyiso_data = {\n",
    "    'regions': regions,\n",
    "    'lat': lat,\n",
    "    'long': long,\n",
    "    'area_km_sq': area_km_sq\n",
    "}\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "    \n",
    "    # Differences in coordinates\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Function to calculate the effective radius based on area\n",
    "def effective_radius(area):\n",
    "    return np.sqrt(area / np.pi)\n",
    "\n",
    "# Prepare the data into a DataFrame\n",
    "nyiso_df = pd.DataFrame(nyiso_data)\n",
    "\n",
    "# Calculate the effective radius for each country\n",
    "nyiso_df['effective_radius'] = nyiso_df['area_km_sq'].apply(effective_radius)\n",
    "\n",
    "# Initialize list to store both normalized and physical distances\n",
    "distances = []\n",
    "\n",
    "# Loop through pairs of countries to calculate distances\n",
    "for i in range(len(nyiso_df)):\n",
    "    for j in range(len(nyiso_df)):\n",
    "        # Get the coordinates, areas and radii\n",
    "        lat1, lon1, r1 = nyiso_df.iloc[i]['lat'], nyiso_df.iloc[i]['long'], nyiso_df.iloc[i]['effective_radius']\n",
    "        lat2, lon2, r2 = nyiso_df.iloc[j]['lat'], nyiso_df.iloc[j]['long'], nyiso_df.iloc[j]['effective_radius']\n",
    "        \n",
    "        # Calculate the physical distance between the centroids using the Haversine formula\n",
    "        distance = haversine(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "        # Calculate the normalized distance\n",
    "        D_norm = distance / (r1 + r2)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        distances.append({\n",
    "            'Region 1': nyiso_df.iloc[i]['regions'],\n",
    "            'Region 2': nyiso_df.iloc[j]['regions'],\n",
    "            'Physical Distance (km)': distance,\n",
    "            'Normalized Distance': D_norm\n",
    "        })\n",
    "\n",
    "distances_df = pd.DataFrame(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_subsets(closest_locations(distances_df, 'Region 1', 'Region 2', 'Normalized Distance', 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_subsets = unique_subsets(closest_locations(distances_df, 'Region 1', 'Region 2', 'Normalized Distance', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(region_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CENTRL', 'GENESE', 'WEST'],\n",
       " ['DUNWOD', 'HUD VL', 'N.Y.C.'],\n",
       " ['DUNWOD', 'HUD VL', 'LONGIL'],\n",
       " ['DUNWOD', 'HUD VL', 'MILLWD'],\n",
       " ['CAPITL', 'MHK VL', 'NORTH'],\n",
       " ['CENTRL', 'GENESE', 'MHK VL'],\n",
       " ['CAPITL', 'CENTRL', 'MHK VL'],\n",
       " ['CAPITL', 'CENTRL', 'GENESE', 'HUD VL', 'MHK VL', 'NORTH'],\n",
       " ['CAPITL', 'CENTRL', 'HUD VL', 'MHK VL', 'NORTH', 'WEST'],\n",
       " ['CAPITL', 'DUNWOD', 'HUD VL', 'LONGIL', 'MHK VL', 'MILLWD'],\n",
       " ['DUNWOD', 'HUD VL', 'LONGIL', 'MHK VL', 'MILLWD', 'N.Y.C.'],\n",
       " ['CAPITL', 'CENTRL', 'GENESE', 'HUD VL', 'MHK VL', 'WEST'],\n",
       " ['CAPITL', 'CENTRL', 'HUD VL', 'MHK VL', 'MILLWD', 'NORTH'],\n",
       " ['CAPITL',\n",
       "  'CENTRL',\n",
       "  'DUNWOD',\n",
       "  'HUD VL',\n",
       "  'LONGIL',\n",
       "  'MHK VL',\n",
       "  'MILLWD',\n",
       "  'N.Y.C.',\n",
       "  'NORTH'],\n",
       " ['CAPITL',\n",
       "  'CENTRL',\n",
       "  'DUNWOD',\n",
       "  'GENESE',\n",
       "  'HUD VL',\n",
       "  'MHK VL',\n",
       "  'MILLWD',\n",
       "  'NORTH',\n",
       "  'WEST'],\n",
       " ['CAPITL',\n",
       "  'CENTRL',\n",
       "  'GENESE',\n",
       "  'HUD VL',\n",
       "  'LONGIL',\n",
       "  'MHK VL',\n",
       "  'MILLWD',\n",
       "  'NORTH',\n",
       "  'WEST'],\n",
       " ['CAPITL',\n",
       "  'CENTRL',\n",
       "  'DUNWOD',\n",
       "  'HUD VL',\n",
       "  'LONGIL',\n",
       "  'MHK VL',\n",
       "  'MILLWD',\n",
       "  'N.Y.C.',\n",
       "  'WEST'],\n",
       " ['CAPITL',\n",
       "  'CENTRL',\n",
       "  'GENESE',\n",
       "  'HUD VL',\n",
       "  'LONGIL',\n",
       "  'MHK VL',\n",
       "  'N.Y.C.',\n",
       "  'NORTH',\n",
       "  'WEST'],\n",
       " ['CAPITL',\n",
       "  'CENTRL',\n",
       "  'DUNWOD',\n",
       "  'GENESE',\n",
       "  'HUD VL',\n",
       "  'LONGIL',\n",
       "  'MHK VL',\n",
       "  'MILLWD',\n",
       "  'N.Y.C.',\n",
       "  'NORTH',\n",
       "  'WEST']]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_subsets = sorted(region_subsets, key=len)\n",
    "region_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time Stamp    0\n",
       "CAPITL        0\n",
       "CENTRL        0\n",
       "DUNWOD        0\n",
       "GENESE        0\n",
       "HUD VL        0\n",
       "LONGIL        0\n",
       "MHK VL        0\n",
       "MILLWD        0\n",
       "N.Y.C.        0\n",
       "NORTH         0\n",
       "WEST          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyiso_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_nyiso(df, selected_columns):\n",
    "    \"\"\"\n",
    "    Filters the given DataFrame to retain only the selected columns, \n",
    "    and removes initial and bottom-most rows where all selected columns are NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        selected_columns (list): List of column names to retain.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure 'Datetime (UTC)' is included in the filtered columns\n",
    "    filtered_columns = ['Time Stamp'] + selected_columns if 'Time Stamp' in df.columns else selected_columns\n",
    "    df_filtered = df[filtered_columns]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CENTRL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_MHK VL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_HUD VL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_LONGIL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_N.Y.C..csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_GENESE.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_NORTH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_DUNWOD.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_MILLWD.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CENTRL_GENESE_WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_DUNWOD_HUD VL_N.Y.C..csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_DUNWOD_HUD VL_LONGIL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_DUNWOD_HUD VL_MILLWD.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_MHK VL_NORTH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CENTRL_GENESE_MHK VL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_MHK VL.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_GENESE_HUD VL_MHK VL_NORTH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_HUD VL_MHK VL_NORTH_WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_DUNWOD_HUD VL_LONGIL_MHK VL_MILLWD.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_DUNWOD_HUD VL_LONGIL_MHK VL_MILLWD_N.Y.C..csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_GENESE_HUD VL_MHK VL_WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_HUD VL_MHK VL_MILLWD_NORTH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_DUNWOD_HUD VL_LONGIL_MHK VL_MILLWD_N.Y.C._NORTH.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_DUNWOD_GENESE_HUD VL_MHK VL_MILLWD_NORTH_WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_GENESE_HUD VL_LONGIL_MHK VL_MILLWD_NORTH_WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_DUNWOD_HUD VL_LONGIL_MHK VL_MILLWD_N.Y.C._WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_GENESE_HUD VL_LONGIL_MHK VL_N.Y.C._NORTH_WEST.csv\n",
      "Saved: C:\\Users\\vm-user\\Downloads\\nyiso_CAPITL_CENTRL_DUNWOD_GENESE_HUD VL_LONGIL_MHK VL_MILLWD_N.Y.C._NORTH_WEST.csv\n"
     ]
    }
   ],
   "source": [
    "nyiso_data_df['Time Stamp'] = pd.to_datetime(nyiso_data_df['Time Stamp'])\n",
    "nyiso_data_df['Time Stamp'] = nyiso_data_df['Time Stamp'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "for region in regions:\n",
    "    df_to_save = filter_nyiso(nyiso_data_df, [region])\n",
    "    output_file_path = f\"C:\\\\Users\\\\vm-user\\\\Downloads\\\\nyiso_{region}.csv\"\n",
    "    # Print a message to see if the df has null values\n",
    "    if df_to_save.isnull().values.any():\n",
    "        print(f\"Warning: DataFrame for {region} contains null values.\")\n",
    "\n",
    "    df_to_save.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved: {output_file_path}\")\n",
    "\n",
    "for subset in region_subsets:\n",
    "    df_to_save = filter_nyiso(nyiso_data_df, subset)\n",
    "    output_file_path = f\"C:\\\\Users\\\\vm-user\\\\Downloads\\\\nyiso_{'_'.join(subset)}.csv\"\n",
    "    # Print a message to see if the df has null values\n",
    "    if df_to_save.isnull().values.any():\n",
    "        print(f\"Warning: DataFrame for {subset} contains null values.\")\n",
    "    df_to_save.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
