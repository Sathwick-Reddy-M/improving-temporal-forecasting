{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74606896",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916cf914",
   "metadata": {},
   "source": [
    "## Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def time_features_util(\n",
    "    dates,\n",
    "    main_df=None,\n",
    "    use_features=[\"year\", \"month\", \"day\", \"weekday\", \"hour\", \"minute\"],\n",
    "    time_col_name=\"Datetime\",\n",
    "):\n",
    "    if main_df is None:\n",
    "        main_df = pd.DataFrame({})\n",
    "    else:\n",
    "        main_df = main_df.copy()\n",
    "    years = dates.apply(lambda row: row.year)\n",
    "    max_year = years.max()\n",
    "    min_year = years.min()\n",
    "\n",
    "    if \"year\" in use_features:\n",
    "        main_df[\"Year\"] = dates.apply(\n",
    "            lambda row: (row.year - min_year) / max(1.0, (max_year - min_year))\n",
    "        )\n",
    "\n",
    "    if \"month\" in use_features:\n",
    "        main_df[\"Month\"] = dates.apply(\n",
    "            lambda row: 2.0 * ((row.month - 1) / 11.0) - 1.0, 1\n",
    "        )\n",
    "    if \"day\" in use_features:\n",
    "        main_df[\"Day\"] = dates.apply(lambda row: 2.0 * ((row.day - 1) / 30.0) - 1.0, 1)\n",
    "    if \"weekday\" in use_features:\n",
    "        main_df[\"Weekday\"] = dates.apply(\n",
    "            lambda row: 2.0 * (row.weekday() / 6.0) - 1.0, 1\n",
    "        )\n",
    "    if \"hour\" in use_features:\n",
    "        main_df[\"Hour\"] = dates.apply(lambda row: 2.0 * ((row.hour) / 23.0) - 1.0, 1)\n",
    "    if \"minute\" in use_features:\n",
    "        main_df[\"Minute\"] = dates.apply(\n",
    "            lambda row: 2.0 * ((row.minute) / 59.0) - 1.0, 1\n",
    "        )\n",
    "\n",
    "    main_df[time_col_name] = dates\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d830d6",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef34fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class DataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        datasetCls,\n",
    "        dataset_kwargs: dict,\n",
    "        batch_size: int,\n",
    "        workers: int,\n",
    "        collate_fn=None,\n",
    "        overfit: bool = False,\n",
    "    ):\n",
    "        self.datasetCls = datasetCls\n",
    "        self.batch_size = batch_size\n",
    "        # Remove 'split' if provided in dataset_kwargs\n",
    "        if \"split\" in dataset_kwargs:\n",
    "            del dataset_kwargs[\"split\"]\n",
    "        self.dataset_kwargs = dataset_kwargs\n",
    "        self.workers = workers\n",
    "        self.collate_fn = collate_fn\n",
    "        if overfit:\n",
    "            warnings.warn(\"Overriding val and test dataloaders to use train set!\")\n",
    "        self.overfit = overfit\n",
    "\n",
    "    def train_dataloader(self, shuffle=True):\n",
    "        return self._make_dloader(\"train\", shuffle=shuffle)\n",
    "\n",
    "    def val_dataloader(self, shuffle=False):\n",
    "        return self._make_dloader(\"val\", shuffle=shuffle)\n",
    "\n",
    "    def test_dataloader(self, shuffle=False):\n",
    "        return self._make_dloader(\"test\", shuffle=shuffle)\n",
    "\n",
    "    def _make_dloader(self, split, shuffle=False):\n",
    "        if self.overfit:\n",
    "            split = \"train\"\n",
    "            shuffle = True\n",
    "        dataset = self.datasetCls(**self.dataset_kwargs, split=split)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(cls, parser):\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\n",
    "            \"--workers\",\n",
    "            type=int,\n",
    "            default=6,\n",
    "            help=\"number of parallel workers for pytorch dataloader\",\n",
    "        )\n",
    "        parser.add_argument(\"--overfit\", action=\"store_true\")\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63038931",
   "metadata": {},
   "source": [
    "## CSVDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CSVTimeSeries:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str = None,\n",
    "        raw_df: pd.DataFrame = None,\n",
    "        target_cols: List[str] = [],\n",
    "        ignore_cols: List[str] = [],\n",
    "        remove_target_from_context_cols: List[str] = [],\n",
    "        time_col_name: str = \"Datetime\",\n",
    "        read_csv_kwargs={},\n",
    "        val_split: float = 0.15,\n",
    "        test_split: float = 0.15,\n",
    "        normalize: bool = True,\n",
    "        drop_all_nan: bool = False,\n",
    "        time_features: List[str] = [\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"day\",\n",
    "            \"weekday\",\n",
    "            \"hour\",\n",
    "            \"minute\",\n",
    "        ],\n",
    "    ):\n",
    "\n",
    "        assert data_path is not None or raw_df is not None\n",
    "\n",
    "        if raw_df is None:\n",
    "            self.data_path = data_path\n",
    "            assert os.path.exists(self.data_path)\n",
    "            raw_df = pd.read_csv(\n",
    "                self.data_path,\n",
    "                **read_csv_kwargs,\n",
    "            )\n",
    "\n",
    "        if drop_all_nan:\n",
    "            raw_df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "        self.time_col_name = time_col_name\n",
    "        assert self.time_col_name in raw_df.columns\n",
    "\n",
    "        if not target_cols:\n",
    "            target_cols = raw_df.columns.tolist()\n",
    "            target_cols.remove(time_col_name)\n",
    "\n",
    "        if ignore_cols:\n",
    "            if ignore_cols == \"all\":\n",
    "                ignore_cols = raw_df.columns.difference(target_cols).tolist()\n",
    "                ignore_cols.remove(self.time_col_name)\n",
    "            raw_df.drop(columns=ignore_cols, inplace=True)\n",
    "\n",
    "        time_df = pd.to_datetime(raw_df[self.time_col_name], format=\"%Y-%m-%d %H:%M\")\n",
    "        df = time_features_util(\n",
    "            time_df,\n",
    "            raw_df,\n",
    "            time_col_name=self.time_col_name,\n",
    "            use_features=time_features,\n",
    "        )\n",
    "        self.time_cols = df.columns.difference(raw_df.columns)\n",
    "\n",
    "        # Train/Val/Test Split using holdout approach #\n",
    "\n",
    "        def mask_intervals(mask, intervals, cond):\n",
    "            for (interval_low, interval_high) in intervals:\n",
    "                if interval_low is None:\n",
    "                    interval_low = df[self.time_col_name].iloc[0].year\n",
    "                if interval_high is None:\n",
    "                    interval_high = df[self.time_col_name].iloc[-1].year\n",
    "                mask[\n",
    "                    (df[self.time_col_name] >= interval_low)\n",
    "                    & (df[self.time_col_name] <= interval_high)\n",
    "                ] = cond\n",
    "            return mask\n",
    "\n",
    "        test_cutoff = len(time_df) - max(round(test_split * len(time_df)), 1)\n",
    "        val_cutoff = test_cutoff - round(val_split * len(time_df))\n",
    "\n",
    "        val_interval_low = time_df.iloc[val_cutoff]\n",
    "        val_interval_high = time_df.iloc[test_cutoff - 1]\n",
    "        val_intervals = [(val_interval_low, val_interval_high)]\n",
    "\n",
    "        test_interval_low = time_df.iloc[test_cutoff]\n",
    "        test_interval_high = time_df.iloc[-1]\n",
    "        test_intervals = [(test_interval_low, test_interval_high)]\n",
    "\n",
    "        train_mask = df[self.time_col_name] > pd.Timestamp.min\n",
    "        val_mask = df[self.time_col_name] > pd.Timestamp.max\n",
    "        test_mask = df[self.time_col_name] > pd.Timestamp.max\n",
    "        train_mask = mask_intervals(train_mask, test_intervals, False)\n",
    "        train_mask = mask_intervals(train_mask, val_intervals, False)\n",
    "        val_mask = mask_intervals(val_mask, val_intervals, True)\n",
    "        test_mask = mask_intervals(test_mask, test_intervals, True)\n",
    "\n",
    "        if (train_mask == False).all():\n",
    "            print(f\"No training data detected for file {data_path}\")\n",
    "\n",
    "        self._train_data = df[train_mask]\n",
    "        self._scaler = StandardScaler()\n",
    "\n",
    "        self.target_cols = target_cols\n",
    "        for col in remove_target_from_context_cols:\n",
    "            assert (\n",
    "                col in self.target_cols\n",
    "            ), \"`remove_target_from_context_cols` should be target cols that you want to remove from the context\"\n",
    "\n",
    "        self.remove_target_from_context_cols = remove_target_from_context_cols\n",
    "        not_exo_cols = self.time_cols.tolist() + target_cols\n",
    "        self.exo_cols = df.columns.difference(not_exo_cols).tolist()\n",
    "        self.exo_cols.remove(self.time_col_name)\n",
    "\n",
    "        self._train_data = df[train_mask]\n",
    "        self._val_data = df[val_mask]\n",
    "        if test_split == 0.0:\n",
    "            print(\"`test_split` set to 0. Using Val set as Test set.\")\n",
    "            self._test_data = df[val_mask]\n",
    "        else:\n",
    "            self._test_data = df[test_mask]\n",
    "\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self._scaler = self._scaler.fit(\n",
    "                self._train_data[target_cols + self.exo_cols].values\n",
    "            )\n",
    "        self._train_data = self.apply_scaling_df(self._train_data)\n",
    "        self._val_data = self.apply_scaling_df(self._val_data)\n",
    "        self._test_data = self.apply_scaling_df(self._test_data)\n",
    "\n",
    "    def make_hists(self):\n",
    "        for col in self.target_cols + self.exo_cols:\n",
    "            train = self._train_data[col]\n",
    "            test = self._test_data[col]\n",
    "            bins = np.linspace(-5, 5, 80)  # warning: edit bucket limits\n",
    "            plt.hist(train, bins, alpha=0.5, label=\"Train\", density=True)\n",
    "            plt.hist(test, bins, alpha=0.5, label=\"Test\", density=True)\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.title(col)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{col}-hist.png\")\n",
    "            plt.clf()\n",
    "\n",
    "    def get_slice(self, split, start, stop, skip):\n",
    "        assert split in [\"train\", \"val\", \"test\"]\n",
    "        if split == \"train\":\n",
    "            return self.train_data.iloc[start:stop:skip]\n",
    "        elif split == \"val\":\n",
    "            return self.val_data.iloc[start:stop:skip]\n",
    "        else:\n",
    "            return self.test_data.iloc[start:stop:skip]\n",
    "\n",
    "    def apply_scaling(self, array):\n",
    "        if not self.normalize:\n",
    "            return array\n",
    "        dim = array.shape[-1]\n",
    "        return (array - self._scaler.mean_[:dim]) / self._scaler.scale_[:dim]\n",
    "\n",
    "    def apply_scaling_df(self, df):\n",
    "        if not self.normalize:\n",
    "            return df\n",
    "        scaled = df.copy(deep=True)\n",
    "        cols = self.target_cols + self.exo_cols\n",
    "        dtype = df[cols].values.dtype\n",
    "        scaled[cols] = (\n",
    "            df[cols].values - self._scaler.mean_.astype(dtype)\n",
    "        ) / self._scaler.scale_.astype(dtype)\n",
    "        return scaled\n",
    "\n",
    "    def reverse_scaling_df(self, df):\n",
    "        if not self.normalize:\n",
    "            return df\n",
    "        scaled = df.copy(deep=True)\n",
    "        cols = self.target_cols + self.exo_cols\n",
    "        dtype = df[cols].values.dtype\n",
    "        scaled[cols] = (\n",
    "            df[cols].values * self._scaler.scale_.astype(dtype)\n",
    "        ) + self._scaler.mean_.astype(dtype)\n",
    "        return scaled\n",
    "\n",
    "    def reverse_scaling(self, array):\n",
    "        if not self.normalize:\n",
    "            return array\n",
    "        # self._scaler is fit for target_cols + exo_cols\n",
    "        # if the array dim is less than this length we start\n",
    "        # slicing from the target cols\n",
    "        dim = array.shape[-1]\n",
    "        return (array * self._scaler.scale_[:dim]) + self._scaler.mean_[:dim]\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        return self._train_data\n",
    "\n",
    "    @property\n",
    "    def val_data(self):\n",
    "        return self._val_data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        return self._test_data\n",
    "\n",
    "    def length(self, split):\n",
    "        return {\n",
    "            \"train\": len(self.train_data),\n",
    "            \"val\": len(self.val_data),\n",
    "            \"test\": len(self.test_data),\n",
    "        }[split]\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(self, parser):\n",
    "        parser.add_argument(\"--data_path\", type=str, default=\"auto\")\n",
    "\n",
    "\n",
    "class CSVTorchDset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_time_series: CSVTimeSeries,\n",
    "        split: str = \"train\",\n",
    "        context_points: int = 128,\n",
    "        target_points: int = 32,\n",
    "        time_resolution: int = 1,\n",
    "    ):\n",
    "        assert split in [\"train\", \"val\", \"test\"]\n",
    "        self.split = split\n",
    "        self.series = csv_time_series\n",
    "        self.context_points = context_points\n",
    "        self.target_points = target_points\n",
    "        self.time_resolution = time_resolution\n",
    "\n",
    "        self._slice_start_points = [\n",
    "            i\n",
    "            for i in range(\n",
    "                0,\n",
    "                self.series.length(split)\n",
    "                + time_resolution * (-target_points - context_points)\n",
    "                + 1,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._slice_start_points)\n",
    "\n",
    "    def _torch(self, *dfs):\n",
    "        return tuple(torch.from_numpy(x.values).float() for x in dfs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = self._slice_start_points[i]\n",
    "        series_slice = self.series.get_slice(\n",
    "            self.split,\n",
    "            start=start,\n",
    "            stop=start\n",
    "            + self.time_resolution * (self.context_points + self.target_points),\n",
    "            skip=self.time_resolution,\n",
    "        )\n",
    "        series_slice = series_slice.drop(columns=[self.series.time_col_name])\n",
    "        ctxt_slice, trgt_slice = (\n",
    "            series_slice.iloc[: self.context_points],\n",
    "            series_slice.iloc[self.context_points :],\n",
    "        )\n",
    "\n",
    "        ctxt_x = ctxt_slice[self.series.time_cols]\n",
    "        trgt_x = trgt_slice[self.series.time_cols]\n",
    "\n",
    "        ctxt_y = ctxt_slice[self.series.target_cols + self.series.exo_cols]\n",
    "        ctxt_y = ctxt_y.drop(columns=self.series.remove_target_from_context_cols)\n",
    "\n",
    "        trgt_y = trgt_slice[self.series.target_cols]\n",
    "\n",
    "        return self._torch(ctxt_x, ctxt_y, trgt_x, trgt_y)\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(self, parser):\n",
    "        parser.add_argument(\n",
    "            \"--context_points\",\n",
    "            type=int,\n",
    "            default=128,\n",
    "            help=\"number of previous timesteps given to the model in order to make predictions\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--target_points\",\n",
    "            type=int,\n",
    "            default=32,\n",
    "            help=\"number of future timesteps to predict\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--time_resolution\",\n",
    "            type=int,\n",
    "            default=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036e60e",
   "metadata": {},
   "source": [
    "# RevIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b2dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reversible Instance Normalization from \n",
    "https://github.com/ts-kim/RevIN\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MovingAvg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeriesDecomposition(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str, update_stats=True):\n",
    "        assert x.ndim == 3\n",
    "        if mode == \"norm\":\n",
    "            if update_stats:\n",
    "                self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == \"denorm\":\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(\n",
    "            torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps\n",
    "        ).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b23f3",
   "metadata": {},
   "source": [
    "# Time2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbd9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, input_dim=6, embed_dim=512, act_function=torch.sin):\n",
    "        assert embed_dim % input_dim == 0\n",
    "        super(Time2Vec, self).__init__()\n",
    "        self.enabled = embed_dim > 0\n",
    "        if self.enabled:\n",
    "            self.embed_dim = embed_dim // input_dim\n",
    "            self.input_dim = input_dim\n",
    "            self.embed_weight = nn.Parameter(torch.randn(self.input_dim, self.embed_dim))\n",
    "            self.embed_bias = nn.Parameter(torch.randn(self.input_dim, self.embed_dim))\n",
    "            self.act_function = act_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.enabled:\n",
    "            x = torch.diag_embed(x)\n",
    "            # x.shape = (bs, sequence_length, input_dim, input_dim)\n",
    "            x_affine = torch.matmul(x, self.embed_weight) + self.embed_bias\n",
    "            # x_affine.shape = (bs, sequence_length, input_dim, time_embed_dim)\n",
    "            x_affine_0, x_affine_remain = torch.split(\n",
    "                x_affine, [1, self.embed_dim - 1], dim=-1\n",
    "            )\n",
    "            x_affine_remain = self.act_function(x_affine_remain)\n",
    "            x_output = torch.cat([x_affine_0, x_affine_remain], dim=-1)\n",
    "            x_output = x_output.view(x_output.size(0), x_output.size(1), -1)\n",
    "            # x_output.shape = (bs, sequence_length, input_dim * time_embed_dim)\n",
    "        else:\n",
    "            x_output = x\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4473493",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "544d1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "class TeacherForcingAnnealCallback:\n",
    "    def __init__(self, start, end, steps):\n",
    "        assert start >= end, \"teacher_forcing_start must be >= teacher_forcing_end\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.slope = float(start - end) / steps\n",
    "\n",
    "    def on_train_batch_end(self, model):\n",
    "        \"\"\"\n",
    "        Update teacher forcing probability after each training batch.\n",
    "        The model is expected to have an attribute 'teacher_forcing_prob'.\n",
    "        \"\"\"\n",
    "        current = model.teacher_forcing_prob\n",
    "        new_teacher_forcing_prob = max(self.end, current - self.slope)\n",
    "        model.teacher_forcing_prob = new_teacher_forcing_prob\n",
    "        # Replace model.log with any custom logging or simply print\n",
    "        print(f\"teacher_forcing_prob: {new_teacher_forcing_prob}\")\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(cls, parser: argparse.ArgumentParser):\n",
    "        parser.add_argument(\"--teacher_forcing_start\", type=float, default=0.8)\n",
    "        parser.add_argument(\"--teacher_forcing_end\", type=float, default=0.0)\n",
    "        parser.add_argument(\"--teacher_forcing_anneal_steps\", type=int, default=8000)\n",
    "        return parser\n",
    "\n",
    "\n",
    "class TimeMaskedLossCallback:\n",
    "    def __init__(self, start, end, steps):\n",
    "        assert start <= end, \"time_mask_start must be <= time_mask_end\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.slope = float(end - start) / steps\n",
    "        self._time_mask = self.start\n",
    "\n",
    "    @property\n",
    "    def time_mask(self):\n",
    "        return round(self._time_mask)\n",
    "\n",
    "    def on_train_start(self, model):\n",
    "        \"\"\"\n",
    "        Set the model's time_masked_idx at the beginning of training if not already set.\n",
    "        \"\"\"\n",
    "        if getattr(model, \"time_masked_idx\", None) is None:\n",
    "            model.time_masked_idx = self.time_mask\n",
    "            print(f\"time_masked_idx set to: {self.time_mask}\")\n",
    "\n",
    "    def on_train_batch_end(self, model):\n",
    "        \"\"\"\n",
    "        Increment the time mask value after each training batch and update the model's attribute.\n",
    "        \"\"\"\n",
    "        self._time_mask = min(self.end, self._time_mask + self.slope)\n",
    "        model.time_masked_idx = self.time_mask\n",
    "        print(f\"time_masked_idx: {self.time_mask}\")\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(cls, parser: argparse.ArgumentParser):\n",
    "        parser.add_argument(\"--time_mask_start\", type=int, default=1)\n",
    "        parser.add_argument(\"--time_mask_end\", type=int, default=12)\n",
    "        parser.add_argument(\"--time_mask_anneal_steps\", type=int, default=1000)\n",
    "        parser.add_argument(\"--time_mask_loss\", action=\"store_true\")\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18ce7e",
   "metadata": {},
   "source": [
    "# Eval Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03453af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EPSILON = 1e-7\n",
    "\n",
    "\n",
    "def r_squared(actual: np.ndarray, predicted: np.ndarray):\n",
    "    rss = (_error(actual, predicted) ** 2).sum(1)\n",
    "    tss = (_error(actual, actual.mean(1, keepdims=True)) ** 2).sum(1)\n",
    "    r2 = 1.0 - rss / (tss + EPSILON)\n",
    "    return r2.mean()\n",
    "\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Simple error\"\"\"\n",
    "    return actual - predicted\n",
    "\n",
    "\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "\n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def mape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "    return np.mean(np.abs(_percentage_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def smape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    return np.mean(\n",
    "        2.0\n",
    "        * np.abs(actual - predicted)\n",
    "        / ((np.abs(actual) + np.abs(predicted)) + EPSILON)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd7556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b691ac89",
   "metadata": {},
   "source": [
    "# Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d957eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Forecaster(nn.Module, ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_x: int,\n",
    "        d_yc: int,\n",
    "        d_yt: int,\n",
    "        learning_rate: float = 1e-3,\n",
    "        l2_coeff: float = 0,\n",
    "        loss: str = \"mse\",\n",
    "        linear_window: int = 0,\n",
    "        linear_shared_weights: bool = False,\n",
    "        use_revin: bool = False,\n",
    "        use_seasonal_decomp: bool = False,\n",
    "        verbose: int = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        qprint = lambda msg: print(msg) if verbose else None\n",
    "        qprint(\"Forecaster\")\n",
    "        qprint(f\"\\tL2: {l2_coeff}\")\n",
    "        qprint(f\"\\tLinear Window: {linear_window}\")\n",
    "        qprint(f\"\\tLinear Shared Weights: {linear_shared_weights}\")\n",
    "        qprint(f\"\\tRevIN: {use_revin}\")\n",
    "        qprint(f\"\\tDecomposition: {use_seasonal_decomp}\")\n",
    "\n",
    "        self._inv_scaler = lambda x: x\n",
    "        self.l2_coeff = l2_coeff\n",
    "        self.learning_rate = learning_rate\n",
    "        self.time_masked_idx = None\n",
    "        self.null_value = None\n",
    "        self.loss = loss\n",
    "\n",
    "        if linear_window:\n",
    "            self.linear_model = LinearModel(\n",
    "                linear_window, shared_weights=linear_shared_weights, d_yt=d_yt\n",
    "            )\n",
    "        else:\n",
    "            self.linear_model = lambda x, *args, **kwargs: 0.0\n",
    "\n",
    "        self.use_revin = use_revin\n",
    "        if use_revin:\n",
    "            assert d_yc == d_yt, \"TODO: figure out exo case for revin\"\n",
    "            self.revin = RevIN(num_features=d_yc)\n",
    "        else:\n",
    "            self.revin = lambda x, **kwargs: x\n",
    "\n",
    "        self.use_seasonal_decomp = use_seasonal_decomp\n",
    "        if use_seasonal_decomp:\n",
    "            self.seasonal_decomp = SeriesDecomposition(kernel_size=25)\n",
    "        else:\n",
    "            self.seasonal_decomp = lambda x: (x, x.clone())\n",
    "\n",
    "        self.d_x = d_x\n",
    "        self.d_yc = d_yc\n",
    "        self.d_yt = d_yt\n",
    "\n",
    "    def set_null_value(self, val: float) -> None:\n",
    "        self.null_value = val\n",
    "\n",
    "    def set_inv_scaler(self, scaler) -> None:\n",
    "        self._inv_scaler = scaler\n",
    "\n",
    "    def set_scaler(self, scaler) -> None:\n",
    "        self._scaler = scaler\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def train_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def eval_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    def loss_fn(self, true: torch.Tensor, preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        true = torch.nan_to_num(true)\n",
    "        if self.loss == \"mse\":\n",
    "            loss = (mask * (true - preds)).square().sum() / max(mask.sum(), 1)\n",
    "        elif self.loss == \"mae\":\n",
    "            loss = torch.abs(mask * (true - preds)).sum() / max(mask.sum(), 1)\n",
    "        elif self.loss == \"smape\":\n",
    "            num = 2.0 * torch.abs(preds - true)\n",
    "            den = torch.abs(preds.detach()) + torch.abs(true) + 1e-5\n",
    "            loss = 100.0 * (mask * (num / den)).sum() / max(mask.sum(), 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized Loss Function: {self.loss}\")\n",
    "        return loss\n",
    "\n",
    "    def forecasting_loss(self, outputs: torch.Tensor, y_t: torch.Tensor, time_mask: int) -> Tuple[torch.Tensor]:\n",
    "        if self.null_value is not None:\n",
    "            null_mask_mat = y_t != self.null_value\n",
    "        else:\n",
    "            null_mask_mat = torch.ones_like(y_t)\n",
    "        null_mask_mat *= ~torch.isnan(y_t)\n",
    "\n",
    "        time_mask_mat = torch.ones_like(y_t)\n",
    "        if time_mask is not None:\n",
    "            time_mask_mat[:, time_mask:] = False\n",
    "\n",
    "        full_mask = time_mask_mat * null_mask_mat\n",
    "        forecasting_loss = self.loss_fn(y_t, outputs, full_mask)\n",
    "        return forecasting_loss, full_mask\n",
    "\n",
    "    def compute_loss(self, batch: Tuple[torch.Tensor], time_mask: int = None, forward_kwargs: dict = {}) -> Tuple[torch.Tensor]:\n",
    "        x_c, y_c, x_t, y_t = batch\n",
    "        outputs, *_ = self.forward(x_c, y_c, x_t, y_t, **forward_kwargs)\n",
    "        loss, mask = self.forecasting_loss(outputs=outputs, y_t=y_t, time_mask=time_mask)\n",
    "        return loss, outputs, mask\n",
    "\n",
    "    def predict(self, x_c: torch.Tensor, y_c: torch.Tensor, x_t: torch.Tensor, sample_preds: bool = False) -> torch.Tensor:\n",
    "        og_device = y_c.device\n",
    "        # Ensure tensors are on the same device as the model.\n",
    "        x_c = x_c.to(next(self.parameters()).device).float()\n",
    "        x_t = x_t.to(next(self.parameters()).device).float()\n",
    "        y_c = torch.from_numpy(self._scaler(y_c.cpu().numpy())).to(next(self.parameters()).device).float()\n",
    "        y_t = torch.zeros((x_t.shape[0], x_t.shape[1], self.d_yt), device=next(self.parameters()).device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            normalized_preds, *_ = self.forward(x_c, y_c, x_t, y_t, **self.eval_step_forward_kwargs)\n",
    "        preds = torch.from_numpy(self._inv_scaler(normalized_preds.cpu().numpy())).to(og_device).float()\n",
    "        return preds\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_model_pass(\n",
    "        self,\n",
    "        x_c: torch.Tensor,\n",
    "        y_c: torch.Tensor,\n",
    "        x_t: torch.Tensor,\n",
    "        y_t: torch.Tensor,\n",
    "        **forward_kwargs,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        return NotImplemented\n",
    "\n",
    "    def nan_to_num(self, *inps):\n",
    "        return (torch.nan_to_num(i) for i in inps)\n",
    "\n",
    "    def forward(self, x_c: torch.Tensor, y_c: torch.Tensor, x_t: torch.Tensor, y_t: torch.Tensor, **forward_kwargs) -> Tuple[torch.Tensor]:\n",
    "        x_c, y_c, x_t, y_t = self.nan_to_num(x_c, y_c, x_t, y_t)\n",
    "        _, pred_len, d_yt = y_t.shape\n",
    "\n",
    "        y_c = self.revin(y_c, mode=\"norm\")\n",
    "        seasonal_yc, trend_yc = self.seasonal_decomp(y_c)\n",
    "        preds, *extra = self.forward_model_pass(x_c, seasonal_yc, x_t, y_t, **forward_kwargs)\n",
    "        baseline = self.linear_model(trend_yc, pred_len=pred_len, d_yt=d_yt)\n",
    "        output = self.revin(preds + baseline, mode=\"denorm\")\n",
    "\n",
    "        if extra:\n",
    "            return (output,) + tuple(extra)\n",
    "        return (output,)\n",
    "\n",
    "    def _compute_stats(self, pred: torch.Tensor, true: torch.Tensor, mask: torch.Tensor):\n",
    "        pred = pred * mask\n",
    "        true = torch.nan_to_num(true) * mask\n",
    "\n",
    "        adj = mask.mean().cpu().numpy() + 1e-5\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        true = true.detach().cpu().numpy()\n",
    "        scaled_pred = self._inv_scaler(pred)\n",
    "        scaled_true = self._inv_scaler(true)\n",
    "        stats = {\n",
    "            \"mape\": mape(scaled_true, scaled_pred) / adj,\n",
    "            \"mae\": mae(scaled_true, scaled_pred) / adj,\n",
    "            \"mse\": mse(scaled_true, scaled_pred) / adj,\n",
    "            \"smape\": smape(scaled_true, scaled_pred) / adj,\n",
    "            \"norm_mae\": mae(true, pred) / adj,\n",
    "            \"norm_mse\": mse(true, pred) / adj,\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def step(self, batch: Tuple[torch.Tensor], train: bool = False):\n",
    "        kwargs = self.train_step_forward_kwargs if train else self.eval_step_forward_kwargs\n",
    "        time_mask = self.time_masked_idx if train else None\n",
    "        loss, output, mask = self.compute_loss(batch=batch, time_mask=time_mask, forward_kwargs=kwargs)\n",
    "        *_, y_t = batch\n",
    "        stats = self._compute_stats(output, y_t, mask)\n",
    "        stats[\"loss\"] = loss\n",
    "        return stats\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, train=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        stats = self.step(batch, train=False)\n",
    "        self.current_val_stats = stats\n",
    "        return stats\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, train=False)\n",
    "\n",
    "    def _log_stats(self, section, outs):\n",
    "        for key in outs.keys():\n",
    "            stat = outs[key]\n",
    "            if isinstance(stat, np.ndarray) or isinstance(stat, torch.Tensor):\n",
    "                stat = stat.mean()\n",
    "            self.log(f\"{section}/{key}\", stat, sync_dist=True)\n",
    "\n",
    "    def training_step_end(self, outs):\n",
    "        self._log_stats(\"train\", outs)\n",
    "        return {\"loss\": outs[\"loss\"].mean()}\n",
    "\n",
    "    def validation_step_end(self, outs):\n",
    "        self._log_stats(\"val\", outs)\n",
    "        return outs\n",
    "\n",
    "    def test_step_end(self, outs):\n",
    "        self._log_stats(\"test\", outs)\n",
    "        return {\"loss\": outs[\"loss\"].mean()}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self(*batch, **self.eval_step_forward_kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.learning_rate, weight_decay=self.l2_coeff\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=3,\n",
    "            factor=0.2,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val/loss\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(self, parser):\n",
    "        parser.add_argument(\"--gpus\", type=int, nargs=\"+\")\n",
    "        parser.add_argument(\"--l2_coeff\", type=float, default=1e-6)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
    "        parser.add_argument(\"--grad_clip_norm\", type=float, default=0)\n",
    "        parser.add_argument(\"--linear_window\", type=int, default=0)\n",
    "        parser.add_argument(\"--use_revin\", action=\"store_true\")\n",
    "        parser.add_argument(\n",
    "            \"--loss\", type=str, default=\"mse\", choices=[\"mse\", \"mae\", \"smape\"]\n",
    "        )\n",
    "        parser.add_argument(\"--linear_shared_weights\", action=\"store_true\")\n",
    "        parser.add_argument(\"--use_seasonal_decomp\", action=\"store_true\")\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5638b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77780ee",
   "metadata": {},
   "source": [
    "# LinearAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e20f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, context_points: int, shared_weights: bool = False, d_yt: int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        if not shared_weights:\n",
    "            assert d_yt is not None\n",
    "            layer_count = d_yt\n",
    "        else:\n",
    "            layer_count = 1\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones((context_points, layer_count)), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones((layer_count)), requires_grad=True)\n",
    "\n",
    "        d = math.sqrt(1.0 / context_points)\n",
    "        self.weights.data.uniform_(-d, d)\n",
    "        self.bias.data.uniform_(-d, d)\n",
    "\n",
    "        self.window = context_points\n",
    "        self.shared_weights = shared_weights\n",
    "        self.d_yt = d_yt\n",
    "\n",
    "    def forward(self, y_c: torch.Tensor, pred_len: int, d_yt: int = None):\n",
    "        batch, length, d_yc = y_c.shape\n",
    "        d_yt = d_yt or self.d_yt\n",
    "\n",
    "        output = torch.zeros(batch, pred_len, d_yt, device=y_c.device)\n",
    "\n",
    "        for i in range(pred_len):\n",
    "            inp = torch.cat((y_c[:, i:, :d_yt], output[:, :i]), dim=1)\n",
    "            output[:, i, :] = self._inner_forward(inp)\n",
    "        return output\n",
    "\n",
    "    def _inner_forward(self, inp):\n",
    "        batch = inp.shape[0]\n",
    "        if self.shared_weights:\n",
    "            inp = rearrange(inp, \"batch length dy -> (batch dy) length 1\")\n",
    "        baseline = (self.weights * inp[:, -self.window :, :]).sum(1) + self.bias\n",
    "        if self.shared_weights:\n",
    "            baseline = rearrange(baseline, \"(batch dy) 1 -> batch dy\", batch=batch)\n",
    "        return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, y_c, target, epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred_len = target.shape[1]\n",
    "        output = model(y_c, pred_len)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eb2fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Linear_Forecaster(Forecaster):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_x: int,\n",
    "        d_yc: int,\n",
    "        d_yt: int,\n",
    "        context_points: int,\n",
    "        learning_rate: float = 1e-3,\n",
    "        l2_coeff: float = 0,\n",
    "        loss: str = \"mse\",\n",
    "        linear_window: int = 0,\n",
    "        linear_shared_weights: bool = False,\n",
    "        use_revin: bool = False,\n",
    "        use_seasonal_decomp: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            d_x=d_x,\n",
    "            d_yc=d_yc,\n",
    "            d_yt=d_yt,\n",
    "            l2_coeff=l2_coeff,\n",
    "            learning_rate=learning_rate,\n",
    "            loss=loss,\n",
    "            linear_window=linear_window,\n",
    "            linear_shared_weights=linear_shared_weights,\n",
    "            use_revin=use_revin,\n",
    "            use_seasonal_decomp=use_seasonal_decomp,\n",
    "        )\n",
    "\n",
    "        self.model = LinearModel(\n",
    "            context_points, shared_weights=linear_shared_weights, d_yt=d_yt\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def eval_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def train_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    def forward_model_pass(self, x_c, y_c, x_t, y_t):\n",
    "        _, pred_len, d_yt = y_t.shape\n",
    "        output = self.model(y_c, pred_len=pred_len, d_yt=d_yt)\n",
    "        return (output,)\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(self, parser):\n",
    "        super().add_cli(parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9499ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3034\n",
      "Epoch 100, Loss: 1.0491\n",
      "Epoch 200, Loss: 0.9557\n",
      "Epoch 300, Loss: 0.9149\n",
      "Epoch 400, Loss: 0.8962\n",
      "Epoch 500, Loss: 0.8877\n",
      "Epoch 600, Loss: 0.8837\n",
      "Epoch 700, Loss: 0.8816\n",
      "Epoch 800, Loss: 0.8803\n",
      "Epoch 900, Loss: 0.8795\n"
     ]
    }
   ],
   "source": [
    "# Example synthetic data\n",
    "batch = 16\n",
    "context_points = 10\n",
    "pred_len = 5\n",
    "d_yt = 7\n",
    "\n",
    "# y_c: context data, target: future data to forecast\n",
    "y_c = torch.randn(batch, context_points, d_yt)\n",
    "target = torch.randn(batch, pred_len, d_yt)\n",
    "\n",
    "model = LinearModel(context_points=context_points, shared_weights=False, d_yt=d_yt)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train(model, optimizer, criterion, y_c, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b35485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETTm1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_sliding_windows(data: np.ndarray, context_points: int, pred_len: int):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (T, d_yt)\n",
    "    returns: y_context (batch, context_points, d_yt) and\n",
    "             y_target (batch, pred_len, d_yt)\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    total_length = data.shape[0]\n",
    "    # Create sliding windows\n",
    "    for i in range(total_length - context_points - pred_len + 1):\n",
    "        X.append(data[i : i + context_points])\n",
    "        Y.append(data[i + context_points : i + context_points + pred_len])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c3342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_context shape: torch.Size([69666, 10, 7])\n",
      "y_target shape: torch.Size([69666, 5, 7])\n",
      "Epoch 0, Loss: 49.0206\n",
      "Epoch 100, Loss: 19.7358\n",
      "Epoch 200, Loss: 6.5635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n",
      "\u001b[0;32m     29\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model using all training examples as one batch.\u001b[39;00m\n",
      "\u001b[1;32m---> 32\u001b[0m train(model, optimizer, criterion, y_context, y_target, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, y_c, target, epochs)\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(y_c, pred_len)\n",
      "\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "\u001b[1;32m----> 8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[0;32m    625\u001b[0m     )\n",
      "\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n",
      "\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n",
      "\u001b[0;32m    628\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n",
      "\u001b[0;32m    348\u001b[0m     tensors,\n",
      "\u001b[0;32m    349\u001b[0m     grad_tensors_,\n",
      "\u001b[0;32m    350\u001b[0m     retain_graph,\n",
      "\u001b[0;32m    351\u001b[0m     create_graph,\n",
      "\u001b[0;32m    352\u001b[0m     inputs,\n",
      "\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[0;32m    355\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n",
      "\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_path = \"S:\\\\spatiotemporal-analysis\\\\ETTm1_modified.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Sort by date (if not already sorted)\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# Use the numeric columns as features (d_yt should equal the number of features: 7)\n",
    "feature_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "data = df[feature_columns].values  # shape (T, 7)\n",
    "\n",
    "# Define context and prediction lengths\n",
    "context_points = 10\n",
    "pred_len = 5\n",
    "\n",
    "# Create sliding windows from the data\n",
    "y_context_np, y_target_np = create_sliding_windows(data, context_points, pred_len)\n",
    "\n",
    "# Convert to torch tensors\n",
    "y_context = torch.tensor(y_context_np, dtype=torch.float32)\n",
    "y_target = torch.tensor(y_target_np, dtype=torch.float32)\n",
    "\n",
    "print(\"y_context shape:\", y_context.shape)  # (batch, context_points, 7)\n",
    "print(\"y_target shape:\", y_target.shape)    # (batch, pred_len, 7)\n",
    "\n",
    "# Create the model, optimizer, and loss function\n",
    "d_yt = len(feature_columns)\n",
    "model = LinearModel(context_points=context_points, shared_weights=False, d_yt=d_yt)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model using all training examples as one batch.\n",
    "train(model, optimizer, criterion, y_context, y_target, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2b637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac863f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4f158a",
   "metadata": {},
   "source": [
    "# Train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d963cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "_MODELS = [\"spacetimeformer\", \"mtgnn\", \"heuristic\", \"lstm\", \"lstnet\", \"linear\", \"s4\"]\n",
    "_DSETS = [\n",
    "    \"asos\",\n",
    "    \"metr-la\",\n",
    "    \"pems-bay\",\n",
    "    \"exchange\",\n",
    "    \"precip\",\n",
    "    \"toy2\",\n",
    "    \"solar_energy\",\n",
    "    \"syn\",\n",
    "    \"mnist\",\n",
    "    \"cifar\",\n",
    "    \"copy\",\n",
    "    \"cont_copy\",\n",
    "    \"m4\",\n",
    "    \"wiki\",\n",
    "    \"ettm1\",\n",
    "    \"weather\",\n",
    "    \"monash\",\n",
    "    \"hangzhou\",\n",
    "    \"traffic\",\n",
    "]\n",
    "\n",
    "def create_parser():\n",
    "    model = sys.argv[1]\n",
    "    dset = sys.argv[2]\n",
    "\n",
    "    # Throw error now before we get confusing parser issues\n",
    "    assert (\n",
    "        model in _MODELS\n",
    "    ), f\"Unrecognized model (`{model}`). Options include: {_MODELS}\"\n",
    "    assert dset in _DSETS, f\"Unrecognized dset (`{dset}`). Options include: {_DSETS}\"\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"model\")\n",
    "    parser.add_argument(\"dset\")\n",
    "\n",
    "    # Only Linear Case is taken with ETTm1\n",
    "    CSVTimeSeries.add_cli(parser)\n",
    "    CSVTorchDset.add_cli(parser)\n",
    "    DataModule.add_cli(parser)\n",
    "\n",
    "    # Only Linear Case is taken with ETTm1\n",
    "\n",
    "    Linear_Forecaster.add_cli(parser)\n",
    "\n",
    "    TimeMaskedLossCallback.add_cli(parser)\n",
    "\n",
    "    parser.add_argument(\"--wandb\", action=\"store_true\")\n",
    "    parser.add_argument(\"--plot\", action=\"store_true\")\n",
    "    parser.add_argument(\"--plot_samples\", type=int, default=8)\n",
    "    parser.add_argument(\"--attn_plot\", action=\"store_true\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "    parser.add_argument(\"--run_name\", type=str, required=True)\n",
    "    parser.add_argument(\"--accumulate\", type=int, default=1)\n",
    "    parser.add_argument(\"--val_check_interval\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--limit_val_batches\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--no_earlystopping\", action=\"store_true\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=5)\n",
    "    parser.add_argument(\n",
    "        \"--trials\", type=int, default=1, help=\"How many consecutive trials to run\"\n",
    "    )\n",
    "\n",
    "    if len(sys.argv) > 3 and sys.argv[3] == \"-h\":\n",
    "        parser.print_help()\n",
    "        sys.exit(0)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def create_model(config):\n",
    "    x_dim, yc_dim, yt_dim = None, None, None\n",
    "    if config.dset == \"metr-la\":\n",
    "        x_dim = 2\n",
    "        yc_dim = 207\n",
    "        yt_dim = 207\n",
    "    elif config.dset == \"pems-bay\":\n",
    "        x_dim = 2\n",
    "        yc_dim = 325\n",
    "        yt_dim = 325\n",
    "    elif config.dset == \"precip\":\n",
    "        x_dim = 2\n",
    "        yc_dim = 49\n",
    "        yt_dim = 49\n",
    "    elif config.dset == \"asos\":\n",
    "        x_dim = 6\n",
    "        yc_dim = 6\n",
    "        yt_dim = 6\n",
    "    elif config.dset == \"solar_energy\":\n",
    "        x_dim = 6\n",
    "        yc_dim = 137\n",
    "        yt_dim = 137\n",
    "    elif config.dset == \"exchange\":\n",
    "        x_dim = 6\n",
    "        yc_dim = 8\n",
    "        yt_dim = 8\n",
    "    elif config.dset == \"toy2\":\n",
    "        x_dim = 6\n",
    "        yc_dim = 20\n",
    "        yt_dim = 20\n",
    "    elif config.dset == \"syn\":\n",
    "        x_dim = 5\n",
    "        yc_dim = 20\n",
    "        yt_dim = 20\n",
    "    elif config.dset == \"mnist\":\n",
    "        x_dim = 1\n",
    "        yc_dim = 28\n",
    "        yt_dim = 28\n",
    "    elif config.dset == \"cifar\":\n",
    "        x_dim = 1\n",
    "        yc_dim = 3\n",
    "        yt_dim = 3\n",
    "    elif config.dset == \"copy\" or config.dset == \"cont_copy\":\n",
    "        x_dim = 1\n",
    "        yc_dim = config.copy_vars\n",
    "        yt_dim = config.copy_vars\n",
    "    elif config.dset == \"m4\":\n",
    "        x_dim = 4\n",
    "        yc_dim = 1\n",
    "        yt_dim = 1\n",
    "    elif config.dset == \"wiki\":\n",
    "        x_dim = 2\n",
    "        yc_dim = 1\n",
    "        yt_dim = 1\n",
    "    elif config.dset == \"monash\":\n",
    "        x_dim = 4\n",
    "        yc_dim = 1\n",
    "        yt_dim = 1\n",
    "    elif config.dset == \"ettm1\":\n",
    "        x_dim = 4\n",
    "        yc_dim = 7\n",
    "        yt_dim = 7\n",
    "    elif config.dset == \"weather\":\n",
    "        x_dim = 3\n",
    "        yc_dim = 21\n",
    "        yt_dim = 21\n",
    "    elif config.dset == \"hangzhou\":\n",
    "        x_dim = 4\n",
    "        yc_dim = 160\n",
    "        yt_dim = 160\n",
    "    elif config.dset == \"traffic\":\n",
    "        x_dim = 2\n",
    "        yc_dim = 862\n",
    "        yt_dim = 862\n",
    "    assert x_dim is not None\n",
    "    assert yc_dim is not None\n",
    "    assert yt_dim is not None\n",
    "\n",
    "    # Only Linear Model Case\n",
    "\n",
    "    if config.model == \"linear\":\n",
    "        forecaster = Linear_Forecaster(\n",
    "            d_x=x_dim,\n",
    "            d_yc=yc_dim,\n",
    "            d_yt=yt_dim,\n",
    "            context_points=config.context_points,\n",
    "            learning_rate=config.learning_rate,\n",
    "            l2_coeff=config.l2_coeff,\n",
    "            loss=config.loss,\n",
    "            linear_window=config.linear_window,\n",
    "            linear_shared_weights=config.linear_shared_weights,\n",
    "            use_revin=config.use_revin,\n",
    "            use_seasonal_decomp=config.use_seasonal_decomp,\n",
    "        )\n",
    "    \n",
    "    return forecaster\n",
    "\n",
    "\n",
    "def create_dset(config):\n",
    "    INV_SCALER = lambda x: x\n",
    "    SCALER = lambda x: x\n",
    "    NULL_VAL = None\n",
    "    PLOT_VAR_IDXS = None\n",
    "    PLOT_VAR_NAMES = None\n",
    "    PAD_VAL = None\n",
    "\n",
    "    if config.dset == \"metr-la\" or config.dset == \"pems-bay\":\n",
    "        if config.dset == \"pems-bay\":\n",
    "            assert (\n",
    "                \"pems_bay\" in config.data_path\n",
    "            ), \"Make sure to switch to the pems-bay file!\"\n",
    "        data = stf.data.metr_la.METR_LA_Data(config.data_path)\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.metr_la.METR_LA_Torch,\n",
    "            dataset_kwargs={\"data\": data},\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        INV_SCALER = data.inverse_scale\n",
    "        SCALER = data.scale\n",
    "        NULL_VAL = 0.0\n",
    "\n",
    "    elif config.dset == \"hangzhou\":\n",
    "        data = stf.data.metro.MetroData(config.data_path)\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.metro.MetroTorch,\n",
    "            dataset_kwargs={\"data\": data},\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        INV_SCALER = data.inverse_scale\n",
    "        SCALER = data.scale\n",
    "        NULL_VAL = 0.0\n",
    "\n",
    "    elif config.dset == \"precip\":\n",
    "        dset = stf.data.precip.GeoDset(dset_dir=config.dset_dir, var=\"precip\")\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.precip.CONUS_Precip,\n",
    "            dataset_kwargs={\n",
    "                \"dset\": dset,\n",
    "                \"context_points\": config.context_points,\n",
    "                \"target_points\": config.target_points,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        NULL_VAL = -1.0\n",
    "    elif config.dset == \"syn\":\n",
    "        dset = stf.data.synthetic.SyntheticData(config.data_path)\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.CSVTorchDset,\n",
    "            dataset_kwargs={\n",
    "                \"csv_time_series\": dset,\n",
    "                \"context_points\": config.context_points,\n",
    "                \"target_points\": config.target_points,\n",
    "                \"time_resolution\": config.time_resolution,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        INV_SCALER = dset.reverse_scaling\n",
    "        SCALER = dset.apply_scaling\n",
    "    elif config.dset in [\"mnist\", \"cifar\"]:\n",
    "        if config.dset == \"mnist\":\n",
    "            config.target_points = 28 - config.context_points\n",
    "            datasetCls = stf.data.image_completion.MNISTDset\n",
    "            PLOT_VAR_IDXS = [18, 24]\n",
    "            PLOT_VAR_NAMES = [\"18th row\", \"24th row\"]\n",
    "        else:\n",
    "            config.target_points = 32 * 32 - config.context_points\n",
    "            datasetCls = stf.data.image_completion.CIFARDset\n",
    "            PLOT_VAR_IDXS = [0]\n",
    "            PLOT_VAR_NAMES = [\"Reds\"]\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=datasetCls,\n",
    "            dataset_kwargs={\"context_points\": config.context_points},\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "    elif config.dset == \"copy\":\n",
    "        # set these manually in case the model needs them\n",
    "        config.context_points = config.copy_length + int(\n",
    "            config.copy_include_lags\n",
    "        )  # seq + lags\n",
    "        config.target_points = config.copy_length\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.copy_task.CopyTaskDset,\n",
    "            dataset_kwargs={\n",
    "                \"length\": config.copy_length,\n",
    "                \"copy_vars\": config.copy_vars,\n",
    "                \"lags\": config.copy_lags,\n",
    "                \"mask_prob\": config.copy_mask_prob,\n",
    "                \"include_lags\": config.copy_include_lags,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "    elif config.dset == \"cont_copy\":\n",
    "        # set these manually in case the model needs them\n",
    "        config.context_points = config.copy_length + int(\n",
    "            config.copy_include_lags\n",
    "        )  # seq + lags\n",
    "        config.target_points = config.copy_length\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.cont_copy_task.ContCopyTaskDset,\n",
    "            dataset_kwargs={\n",
    "                \"length\": config.copy_length,\n",
    "                \"copy_vars\": config.copy_vars,\n",
    "                \"lags\": config.copy_lags,\n",
    "                \"include_lags\": config.copy_include_lags,\n",
    "                \"magnitude_matters\": config.copy_mag_matters,\n",
    "                \"freq_shift\": config.copy_freq_shift,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "    elif config.dset == \"m4\":\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.m4.M4TorchDset,\n",
    "            dataset_kwargs={\n",
    "                \"data_path\": config.data_path,\n",
    "                \"resolutions\": args.resolutions,\n",
    "                \"max_len\": args.max_len,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            collate_fn=stf.data.m4.pad_m4_collate,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        NULL_VAL = -1.0\n",
    "        PAD_VAL = -1.0\n",
    "\n",
    "    elif config.dset == \"wiki\":\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            stf.data.wiki.WikipediaTorchDset,\n",
    "            dataset_kwargs={\n",
    "                \"data_path\": config.data_path,\n",
    "                \"forecast_duration\": args.forecast_duration,\n",
    "                \"max_len\": args.max_len,\n",
    "            },\n",
    "            batch_size=args.batch_size,\n",
    "            workers=args.workers,\n",
    "            collate_fn=stf.data.wiki.pad_wiki_collate,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        NULL_VAL = -1.0\n",
    "        PAD_VAL = -1.0\n",
    "        SCALER = stf.data.wiki.WikipediaTorchDset.scale\n",
    "        INV_SCALER = stf.data.wiki.WikipediaTorchDset.inverse_scale\n",
    "    elif config.dset == \"monash\":\n",
    "        root_dir = config.root_dir\n",
    "        DATA_MODULE = stf.data.monash.monash_dloader.make_monash_dmodule(\n",
    "            root_dir=root_dir,\n",
    "            max_len=config.max_len,\n",
    "            include=config.include,\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=config.overfit,\n",
    "        )\n",
    "        NULL_VAL = -64.0\n",
    "        PAD_VAL = -64.0\n",
    "    elif config.dset == \"ettm1\":\n",
    "        target_cols = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "        dset = stf.data.CSVTimeSeries(\n",
    "            data_path=config.data_path,\n",
    "            target_cols=target_cols,\n",
    "            ignore_cols=[],\n",
    "            val_split=4.0 / 20,  # from informer\n",
    "            test_split=4.0 / 20,  # from informer\n",
    "            time_col_name=\"date\",\n",
    "            time_features=[\"month\", \"day\", \"weekday\", \"hour\"],\n",
    "        )\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.CSVTorchDset,\n",
    "            dataset_kwargs={\n",
    "                \"csv_time_series\": dset,\n",
    "                \"context_points\": config.context_points,\n",
    "                \"target_points\": config.target_points,\n",
    "                \"time_resolution\": config.time_resolution,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        INV_SCALER = dset.reverse_scaling\n",
    "        SCALER = dset.apply_scaling\n",
    "        NULL_VAL = None\n",
    "        # PAD_VAL = -32.0\n",
    "        PLOT_VAR_NAMES = target_cols\n",
    "        PLOT_VAR_IDXS = [i for i in range(len(target_cols))]\n",
    "    elif config.dset == \"weather\":\n",
    "        data_path = config.data_path\n",
    "        dset = stf.data.CSVTimeSeries(\n",
    "            data_path=config.data_path,\n",
    "            target_cols=[],\n",
    "            ignore_cols=[],\n",
    "            # paper says 7:1:2 split\n",
    "            val_split=1.0 / 10,\n",
    "            test_split=2.0 / 10,\n",
    "            time_col_name=\"date\",\n",
    "            time_features=[\"day\", \"hour\", \"minute\"],\n",
    "        )\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.CSVTorchDset,\n",
    "            dataset_kwargs={\n",
    "                \"csv_time_series\": dset,\n",
    "                \"context_points\": config.context_points,\n",
    "                \"target_points\": config.target_points,\n",
    "                \"time_resolution\": config.time_resolution,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        INV_SCALER = dset.reverse_scaling\n",
    "        SCALER = dset.apply_scaling\n",
    "        NULL_VAL = None\n",
    "        PLOT_VAR_NAMES = [\"OT\", \"p (mbar)\", \"raining (s)\"]\n",
    "        PLOT_VAR_IDXS = [20, 0, 15]\n",
    "    else:\n",
    "        time_col_name = \"Datetime\"\n",
    "        data_path = config.data_path\n",
    "        time_features = [\"year\", \"month\", \"day\", \"weekday\", \"hour\", \"minute\"]\n",
    "        if config.dset == \"asos\":\n",
    "            if data_path == \"auto\":\n",
    "                data_path = \"./data/temperature-v1.csv\"\n",
    "            target_cols = [\"ABI\", \"AMA\", \"ACT\", \"ALB\", \"JFK\", \"LGA\"]\n",
    "        elif config.dset == \"solar_energy\":\n",
    "            if data_path == \"auto\":\n",
    "                data_path = \"./data/solar_AL_converted.csv\"\n",
    "            target_cols = [str(i) for i in range(137)]\n",
    "        elif \"toy\" in config.dset:\n",
    "            if data_path == \"auto\":\n",
    "                if config.dset == \"toy2\":\n",
    "                    data_path = \"./data/toy_dset2.csv\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unrecognized toy dataset {config.dset}\")\n",
    "            target_cols = [f\"D{i}\" for i in range(1, 21)]\n",
    "        elif config.dset == \"exchange\":\n",
    "            if data_path == \"auto\":\n",
    "                data_path = \"./data/exchange_rate_converted.csv\"\n",
    "            target_cols = [\n",
    "                \"Australia\",\n",
    "                \"United Kingdom\",\n",
    "                \"Canada\",\n",
    "                \"Switzerland\",\n",
    "                \"China\",\n",
    "                \"Japan\",\n",
    "                \"New Zealand\",\n",
    "                \"Singapore\",\n",
    "            ]\n",
    "        elif config.dset == \"traffic\":\n",
    "            if data_path == \"auto\":\n",
    "                data_path = \"./data/traffic.csv\"\n",
    "            target_cols = [f\"Lane {i}\" for i in range(862)]\n",
    "            time_col_name = \"FakeTime\"\n",
    "            time_features = [\"month\", \"day\"]\n",
    "\n",
    "        dset = stf.data.CSVTimeSeries(\n",
    "            data_path=data_path,\n",
    "            target_cols=target_cols,\n",
    "            ignore_cols=\"all\",\n",
    "            time_col_name=time_col_name,\n",
    "            time_features=time_features,\n",
    "            val_split=0.2,\n",
    "            test_split=0.2,\n",
    "        )\n",
    "        DATA_MODULE = stf.data.DataModule(\n",
    "            datasetCls=stf.data.CSVTorchDset,\n",
    "            dataset_kwargs={\n",
    "                \"csv_time_series\": dset,\n",
    "                \"context_points\": config.context_points,\n",
    "                \"target_points\": config.target_points,\n",
    "                \"time_resolution\": config.time_resolution,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=args.overfit,\n",
    "        )\n",
    "        INV_SCALER = dset.reverse_scaling\n",
    "        SCALER = dset.apply_scaling\n",
    "        NULL_VAL = None\n",
    "\n",
    "    return (\n",
    "        DATA_MODULE,\n",
    "        INV_SCALER,\n",
    "        SCALER,\n",
    "        NULL_VAL,\n",
    "        PLOT_VAR_IDXS,\n",
    "        PLOT_VAR_NAMES,\n",
    "        PAD_VAL,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_callbacks(config, save_dir):\n",
    "    filename = f\"{config.run_name}_\" + str(uuid.uuid1()).split(\"-\")[0]\n",
    "    model_ckpt_dir = os.path.join(save_dir, filename)\n",
    "    config.model_ckpt_dir = model_ckpt_dir\n",
    "    saving = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=model_ckpt_dir,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        filename=f\"{config.run_name}\" + \"{epoch:02d}\",\n",
    "        save_top_k=1,\n",
    "        auto_insert_metric_name=True,\n",
    "    )\n",
    "    callbacks = [saving]\n",
    "\n",
    "    if not config.no_earlystopping:\n",
    "        callbacks.append(\n",
    "            pl.callbacks.early_stopping.EarlyStopping(\n",
    "                monitor=\"val/loss\",\n",
    "                patience=config.patience,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if config.wandb:\n",
    "        callbacks.append(pl.callbacks.LearningRateMonitor())\n",
    "\n",
    "    if config.model == \"lstm\":\n",
    "        callbacks.append(\n",
    "            stf.callbacks.TeacherForcingAnnealCallback(\n",
    "                start=config.teacher_forcing_start,\n",
    "                end=config.teacher_forcing_end,\n",
    "                steps=config.teacher_forcing_anneal_steps,\n",
    "            )\n",
    "        )\n",
    "    if config.time_mask_loss:\n",
    "        callbacks.append(\n",
    "            stf.callbacks.TimeMaskedLossCallback(\n",
    "                start=config.time_mask_start,\n",
    "                end=config.time_mask_end,\n",
    "                steps=config.time_mask_anneal_steps,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c2132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
