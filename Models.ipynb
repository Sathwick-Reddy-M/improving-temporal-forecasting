{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RevIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reversible Instance Normalization from \n",
    "https://github.com/ts-kim/RevIN\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MovingAvg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeriesDecomposition(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str, update_stats=True):\n",
    "        assert x.ndim == 3\n",
    "        if mode == \"norm\":\n",
    "            if update_stats:\n",
    "                self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == \"denorm\":\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(\n",
    "            torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps\n",
    "        ).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, input_dim=6, embed_dim=512, act_function=torch.sin):\n",
    "        assert embed_dim % input_dim == 0\n",
    "        super(Time2Vec, self).__init__()\n",
    "        self.enabled = embed_dim > 0\n",
    "        if self.enabled:\n",
    "            self.embed_dim = embed_dim // input_dim\n",
    "            self.input_dim = input_dim\n",
    "            self.embed_weight = nn.Parameter(torch.randn(self.input_dim, self.embed_dim))\n",
    "            self.embed_bias = nn.Parameter(torch.randn(self.input_dim, self.embed_dim))\n",
    "            self.act_function = act_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.enabled:\n",
    "            x = torch.diag_embed(x)\n",
    "            # x.shape = (bs, sequence_length, input_dim, input_dim)\n",
    "            x_affine = torch.matmul(x, self.embed_weight) + self.embed_bias\n",
    "            # x_affine.shape = (bs, sequence_length, input_dim, time_embed_dim)\n",
    "            x_affine_0, x_affine_remain = torch.split(\n",
    "                x_affine, [1, self.embed_dim - 1], dim=-1\n",
    "            )\n",
    "            x_affine_remain = self.act_function(x_affine_remain)\n",
    "            x_output = torch.cat([x_affine_0, x_affine_remain], dim=-1)\n",
    "            x_output = x_output.view(x_output.size(0), x_output.size(1), -1)\n",
    "            # x_output.shape = (bs, sequence_length, input_dim * time_embed_dim)\n",
    "        else:\n",
    "            x_output = x\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EPSILON = 1e-7\n",
    "\n",
    "\n",
    "def r_squared(actual: np.ndarray, predicted: np.ndarray):\n",
    "    rss = (_error(actual, predicted) ** 2).sum(1)\n",
    "    tss = (_error(actual, actual.mean(1, keepdims=True)) ** 2).sum(1)\n",
    "    r2 = 1.0 - rss / (tss + EPSILON)\n",
    "    return r2.mean()\n",
    "\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Simple error\"\"\"\n",
    "    return actual - predicted\n",
    "\n",
    "\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "\n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def mape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "    return np.mean(np.abs(_percentage_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def smape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    return np.mean(\n",
    "        2.0\n",
    "        * np.abs(actual - predicted)\n",
    "        / ((np.abs(actual) + np.abs(predicted)) + EPSILON)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from einops import rearrange\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, context_points: int, shared_weights: bool = False, d_yt: int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        if not shared_weights:\n",
    "            assert d_yt is not None\n",
    "            layer_count = d_yt\n",
    "        else:\n",
    "            layer_count = 1\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones((context_points, layer_count)), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones((layer_count)), requires_grad=True)\n",
    "\n",
    "        d = math.sqrt(1.0 / context_points)\n",
    "        self.weights.data.uniform_(-d, d)\n",
    "        self.bias.data.uniform_(-d, d)\n",
    "\n",
    "        self.window = context_points\n",
    "        self.shared_weights = shared_weights\n",
    "        self.d_yt = d_yt\n",
    "\n",
    "    def forward(self, y_c: torch.Tensor, pred_len: int, d_yt: int = None):\n",
    "        batch, length, d_yc = y_c.shape\n",
    "        d_yt = d_yt or self.d_yt\n",
    "\n",
    "        output = torch.zeros(batch, pred_len, d_yt, device=y_c.device)\n",
    "\n",
    "        for i in range(pred_len):\n",
    "            inp = torch.cat((y_c[:, i:, :d_yt], output[:, :i]), dim=1)\n",
    "            output[:, i, :] = self._inner_forward(inp)\n",
    "        return output\n",
    "\n",
    "    def _inner_forward(self, inp):\n",
    "        batch = inp.shape[0]\n",
    "        if self.shared_weights:\n",
    "            inp = rearrange(inp, \"batch length dy -> (batch dy) length 1\")\n",
    "        baseline = (self.weights * inp[:, -self.window :, :]).sum(1) + self.bias\n",
    "        if self.shared_weights:\n",
    "            baseline = rearrange(baseline, \"(batch dy) 1 -> batch dy\", batch=batch)\n",
    "        return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, y_c, target, epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred_len = target.shape[1]\n",
    "        output = model(y_c, pred_len)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETTm1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_sliding_windows(data: np.ndarray, context_points: int, pred_len: int):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (T, d_yt)\n",
    "    returns: y_context (batch, context_points, d_yt) and\n",
    "             y_target (batch, pred_len, d_yt)\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    total_length = data.shape[0]\n",
    "    # Create sliding windows\n",
    "    for i in range(total_length - context_points - pred_len + 1):\n",
    "        X.append(data[i : i + context_points])\n",
    "        Y.append(data[i + context_points : i + context_points + pred_len])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_context shape: torch.Size([69666, 10, 7])\n",
      "y_target shape: torch.Size([69666, 5, 7])\n",
      "Epoch 0, Loss: 70.6213\n",
      "Epoch 100, Loss: 7.1133\n",
      "Epoch 200, Loss: 4.4918\n",
      "Epoch 300, Loss: 4.1479\n",
      "Epoch 400, Loss: 3.7757\n",
      "Epoch 500, Loss: 3.4020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model using all training examples as one batch.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m train(model, optimizer, criterion, y_context, y_target, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, y_c, target, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(y_c, pred_len)\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m----> 8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_path = \"S:\\\\spatiotemporal-analysis\\\\ETTm1_modified.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Sort by date (if not already sorted)\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# Use the numeric columns as features (d_yt should equal the number of features: 7)\n",
    "feature_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "data = df[feature_columns].values  # shape (T, 7)\n",
    "\n",
    "# Define context and prediction lengths\n",
    "context_points = 10\n",
    "pred_len = 5\n",
    "\n",
    "# Create sliding windows from the data\n",
    "y_context_np, y_target_np = create_sliding_windows(data, context_points, pred_len)\n",
    "\n",
    "# Convert to torch tensors\n",
    "y_context = torch.tensor(y_context_np, dtype=torch.float32)\n",
    "y_target = torch.tensor(y_target_np, dtype=torch.float32)\n",
    "\n",
    "print(\"y_context shape:\", y_context.shape)  # (batch, context_points, 7)\n",
    "print(\"y_target shape:\", y_target.shape)    # (batch, pred_len, 7)\n",
    "\n",
    "# Create the model, optimizer, and loss function\n",
    "d_yt = len(feature_columns)\n",
    "model = LinearModel(context_points=context_points, shared_weights=False, d_yt=d_yt)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model using all training examples as one batch.\n",
    "train(model, optimizer, criterion, y_context, y_target, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecaster - The parent class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Forecaster(nn.Module, ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_x: int,\n",
    "        d_yc: int,\n",
    "        d_yt: int,\n",
    "        learning_rate: float = 1e-3,\n",
    "        l2_coeff: float = 0,\n",
    "        loss: str = \"mse\",\n",
    "        linear_window: int = 0,\n",
    "        linear_shared_weights: bool = False,\n",
    "        use_revin: bool = False,\n",
    "        use_seasonal_decomp: bool = False,\n",
    "        verbose: int = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        qprint = lambda msg: print(msg) if verbose else None\n",
    "        qprint(\"Forecaster\")\n",
    "        qprint(f\"\\tL2: {l2_coeff}\")\n",
    "        qprint(f\"\\tLinear Window: {linear_window}\")\n",
    "        qprint(f\"\\tLinear Shared Weights: {linear_shared_weights}\")\n",
    "        qprint(f\"\\tRevIN: {use_revin}\")\n",
    "        qprint(f\"\\tDecomposition: {use_seasonal_decomp}\")\n",
    "\n",
    "        self._inv_scaler = lambda x: x\n",
    "        self.l2_coeff = l2_coeff\n",
    "        self.learning_rate = learning_rate\n",
    "        self.time_masked_idx = None\n",
    "        self.null_value = None\n",
    "        self.loss = loss\n",
    "\n",
    "        if linear_window:\n",
    "            self.linear_model = LinearModel(\n",
    "                linear_window, shared_weights=linear_shared_weights, d_yt=d_yt\n",
    "            )\n",
    "        else:\n",
    "            self.linear_model = lambda x, *args, **kwargs: 0.0\n",
    "\n",
    "        self.use_revin = use_revin\n",
    "        if use_revin:\n",
    "            assert d_yc == d_yt, \"TODO: figure out exo case for revin\"\n",
    "            self.revin = RevIN(num_features=d_yc)\n",
    "        else:\n",
    "            self.revin = lambda x, **kwargs: x\n",
    "\n",
    "        self.use_seasonal_decomp = use_seasonal_decomp\n",
    "        if use_seasonal_decomp:\n",
    "            self.seasonal_decomp = SeriesDecomposition(kernel_size=25)\n",
    "        else:\n",
    "            self.seasonal_decomp = lambda x: (x, x.clone())\n",
    "\n",
    "        self.d_x = d_x\n",
    "        self.d_yc = d_yc\n",
    "        self.d_yt = d_yt\n",
    "\n",
    "    def set_null_value(self, val: float) -> None:\n",
    "        self.null_value = val\n",
    "\n",
    "    def set_inv_scaler(self, scaler) -> None:\n",
    "        self._inv_scaler = scaler\n",
    "\n",
    "    def set_scaler(self, scaler) -> None:\n",
    "        self._scaler = scaler\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def train_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def eval_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    def loss_fn(self, true: torch.Tensor, preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        true = torch.nan_to_num(true)\n",
    "        if self.loss == \"mse\":\n",
    "            loss = (mask * (true - preds)).square().sum() / max(mask.sum(), 1)\n",
    "        elif self.loss == \"mae\":\n",
    "            loss = torch.abs(mask * (true - preds)).sum() / max(mask.sum(), 1)\n",
    "        elif self.loss == \"smape\":\n",
    "            num = 2.0 * torch.abs(preds - true)\n",
    "            den = torch.abs(preds.detach()) + torch.abs(true) + 1e-5\n",
    "            loss = 100.0 * (mask * (num / den)).sum() / max(mask.sum(), 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized Loss Function: {self.loss}\")\n",
    "        return loss\n",
    "\n",
    "    def forecasting_loss(self, outputs: torch.Tensor, y_t: torch.Tensor, time_mask: int) -> Tuple[torch.Tensor]:\n",
    "        if self.null_value is not None:\n",
    "            null_mask_mat = y_t != self.null_value\n",
    "        else:\n",
    "            null_mask_mat = torch.ones_like(y_t)\n",
    "        null_mask_mat *= ~torch.isnan(y_t)\n",
    "\n",
    "        time_mask_mat = torch.ones_like(y_t)\n",
    "        if time_mask is not None:\n",
    "            time_mask_mat[:, time_mask:] = False\n",
    "\n",
    "        full_mask = time_mask_mat * null_mask_mat\n",
    "        forecasting_loss = self.loss_fn(y_t, outputs, full_mask)\n",
    "        return forecasting_loss, full_mask\n",
    "\n",
    "    def compute_loss(self, batch: Tuple[torch.Tensor], time_mask: int = None, forward_kwargs: dict = {}) -> Tuple[torch.Tensor]:\n",
    "        x_c, y_c, x_t, y_t = batch\n",
    "        outputs, *_ = self.forward(x_c, y_c, x_t, y_t, **forward_kwargs)\n",
    "        loss, mask = self.forecasting_loss(outputs=outputs, y_t=y_t, time_mask=time_mask)\n",
    "        return loss, outputs, mask\n",
    "\n",
    "    def predict(self, x_c: torch.Tensor, y_c: torch.Tensor, x_t: torch.Tensor, sample_preds: bool = False) -> torch.Tensor:\n",
    "        og_device = y_c.device\n",
    "        # Ensure tensors are on the same device as the model.\n",
    "        x_c = x_c.to(next(self.parameters()).device).float()\n",
    "        x_t = x_t.to(next(self.parameters()).device).float()\n",
    "        y_c = torch.from_numpy(self._scaler(y_c.cpu().numpy())).to(next(self.parameters()).device).float()\n",
    "        y_t = torch.zeros((x_t.shape[0], x_t.shape[1], self.d_yt), device=next(self.parameters()).device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            normalized_preds, *_ = self.forward(x_c, y_c, x_t, y_t, **self.eval_step_forward_kwargs)\n",
    "        preds = torch.from_numpy(self._inv_scaler(normalized_preds.cpu().numpy())).to(og_device).float()\n",
    "        return preds\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_model_pass(\n",
    "        self,\n",
    "        x_c: torch.Tensor,\n",
    "        y_c: torch.Tensor,\n",
    "        x_t: torch.Tensor,\n",
    "        y_t: torch.Tensor,\n",
    "        **forward_kwargs,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        return NotImplemented\n",
    "\n",
    "    def nan_to_num(self, *inps):\n",
    "        return (torch.nan_to_num(i) for i in inps)\n",
    "\n",
    "    def forward(self, x_c: torch.Tensor, y_c: torch.Tensor, x_t: torch.Tensor, y_t: torch.Tensor, **forward_kwargs) -> Tuple[torch.Tensor]:\n",
    "        x_c, y_c, x_t, y_t = self.nan_to_num(x_c, y_c, x_t, y_t)\n",
    "        _, pred_len, d_yt = y_t.shape\n",
    "\n",
    "        y_c = self.revin(y_c, mode=\"norm\")\n",
    "        seasonal_yc, trend_yc = self.seasonal_decomp(y_c)\n",
    "        preds, *extra = self.forward_model_pass(x_c, seasonal_yc, x_t, y_t, **forward_kwargs)\n",
    "        baseline = self.linear_model(trend_yc, pred_len=pred_len, d_yt=d_yt)\n",
    "        output = self.revin(preds + baseline, mode=\"denorm\")\n",
    "\n",
    "        if extra:\n",
    "            return (output,) + tuple(extra)\n",
    "        return (output,)\n",
    "\n",
    "    def _compute_stats(self, pred: torch.Tensor, true: torch.Tensor, mask: torch.Tensor):\n",
    "        pred = pred * mask\n",
    "        true = torch.nan_to_num(true) * mask\n",
    "\n",
    "        adj = mask.mean().cpu().numpy() + 1e-5\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        true = true.detach().cpu().numpy()\n",
    "        scaled_pred = self._inv_scaler(pred)\n",
    "        scaled_true = self._inv_scaler(true)\n",
    "        stats = {\n",
    "            \"mape\": mape(scaled_true, scaled_pred) / adj,\n",
    "            \"mae\": mae(scaled_true, scaled_pred) / adj,\n",
    "            \"mse\": mse(scaled_true, scaled_pred) / adj,\n",
    "            \"smape\": smape(scaled_true, scaled_pred) / adj,\n",
    "            \"norm_mae\": mae(true, pred) / adj,\n",
    "            \"norm_mse\": mse(true, pred) / adj,\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def step(self, batch: Tuple[torch.Tensor], train: bool = False):\n",
    "        kwargs = self.train_step_forward_kwargs if train else self.eval_step_forward_kwargs\n",
    "        time_mask = self.time_masked_idx if train else None\n",
    "        loss, output, mask = self.compute_loss(batch=batch, time_mask=time_mask, forward_kwargs=kwargs)\n",
    "        *_, y_t = batch\n",
    "        stats = self._compute_stats(output, y_t, mask)\n",
    "        stats[\"loss\"] = loss\n",
    "        return stats\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, train=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        stats = self.step(batch, train=False)\n",
    "        self.current_val_stats = stats\n",
    "        return stats\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, train=False)\n",
    "\n",
    "    def _log_stats(self, section, outs):\n",
    "        for key in outs.keys():\n",
    "            stat = outs[key]\n",
    "            if isinstance(stat, np.ndarray) or isinstance(stat, torch.Tensor):\n",
    "                stat = stat.mean()\n",
    "            self.log(f\"{section}/{key}\", stat, sync_dist=True)\n",
    "\n",
    "    def training_step_end(self, outs):\n",
    "        self._log_stats(\"train\", outs)\n",
    "        return {\"loss\": outs[\"loss\"].mean()}\n",
    "\n",
    "    def validation_step_end(self, outs):\n",
    "        self._log_stats(\"val\", outs)\n",
    "        return outs\n",
    "\n",
    "    def test_step_end(self, outs):\n",
    "        self._log_stats(\"test\", outs)\n",
    "        return {\"loss\": outs[\"loss\"].mean()}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self(*batch, **self.eval_step_forward_kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.learning_rate, weight_decay=self.l2_coeff\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=3,\n",
    "            factor=0.2,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val/loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearAR Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Linear_Forecaster(Forecaster):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_x: int,\n",
    "        d_yc: int,\n",
    "        d_yt: int,\n",
    "        context_points: int,\n",
    "        learning_rate: float = 1e-3,\n",
    "        l2_coeff: float = 0,\n",
    "        loss: str = \"mse\",\n",
    "        linear_window: int = 0,\n",
    "        linear_shared_weights: bool = False,\n",
    "        use_revin: bool = False,\n",
    "        use_seasonal_decomp: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            d_x=d_x,\n",
    "            d_yc=d_yc,\n",
    "            d_yt=d_yt,\n",
    "            l2_coeff=l2_coeff,\n",
    "            learning_rate=learning_rate,\n",
    "            loss=loss,\n",
    "            linear_window=linear_window,\n",
    "            linear_shared_weights=linear_shared_weights,\n",
    "            use_revin=use_revin,\n",
    "            use_seasonal_decomp=use_seasonal_decomp,\n",
    "        )\n",
    "\n",
    "        self.model = LinearModel(\n",
    "            context_points, shared_weights=linear_shared_weights, d_yt=d_yt\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def eval_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def train_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    def forward_model_pass(self, x_c, y_c, x_t, y_t):\n",
    "        _, pred_len, d_yt = y_t.shape\n",
    "        output = self.model(y_c, pred_len=pred_len, d_yt=d_yt)\n",
    "        return (output,)\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(self, parser):\n",
    "        super().add_cli(parser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETTm1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_sliding_windows(data: np.ndarray, context_points: int, pred_len: int):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (T, d_yt)\n",
    "    returns: y_context (batch, context_points, d_yt) and\n",
    "             y_target (batch, pred_len, d_yt)\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    total_length = data.shape[0]\n",
    "    # Create sliding windows\n",
    "    for i in range(total_length - context_points - pred_len + 1):\n",
    "        X.append(data[i : i + context_points])\n",
    "        Y.append(data[i + context_points : i + context_points + pred_len])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"S:\\\\spatiotemporal-analysis\\\\ETTm1_modified.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Sort by date (if not already sorted)\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# Use the numeric columns as features (d_yt should equal the number of features: 7)\n",
    "feature_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "data = df[feature_columns].values  # shape (T, 7)\n",
    "\n",
    "# Define context and prediction lengths\n",
    "context_points = 10\n",
    "pred_len = 5\n",
    "\n",
    "# Create sliding windows from the data\n",
    "y_context_np, y_target_np = create_sliding_windows(data, context_points, pred_len)\n",
    "\n",
    "# Convert to torch tensors\n",
    "y_context = torch.tensor(y_context_np, dtype=torch.float32)\n",
    "y_target = torch.tensor(y_target_np, dtype=torch.float32)\n",
    "\n",
    "print(\"y_context shape:\", y_context.shape)  # (batch, context_points, 7)\n",
    "print(\"y_target shape:\", y_target.shape)    # (batch, pred_len, 7)\n",
    "\n",
    "# Create the model, optimizer, and loss function\n",
    "d_yt = len(feature_columns)\n",
    "model = LinearModel(context_points=context_points, shared_weights=False, d_yt=d_yt)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model using all training examples as one batch.\n",
    "train(model, optimizer, criterion, y_context, y_target, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_context shape: torch.Size([69666, 10, 7])\n",
      "y_target shape: torch.Size([69666, 5, 7])\n",
      "Forecaster\n",
      "\tL2: 0\n",
      "\tLinear Window: 0\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: True\n",
      "\tDecomposition: False\n",
      "Epoch 100, Loss: 4.463812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m train(model, optimizer, criterion, x_context, y_context, x_target, y_target, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, x_c, y_c, x_t, y_t, epochs)\u001b[0m\n\u001b[0;32m     64\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, y_t)\n\u001b[1;32m---> 66\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Read CSV and prepare data\n",
    "csv_path = \"S:\\\\spatiotemporal-analysis\\\\ETTm1_modified.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Sort by date (if not already sorted)\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# Use the numeric columns as features (d_yt should equal the number of features: 7)\n",
    "feature_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "data = df[feature_columns].values  # shape (T, 7)\n",
    "\n",
    "# Define context and prediction lengths\n",
    "context_points = 10\n",
    "pred_len = 5\n",
    "\n",
    "# Create sliding windows from the data\n",
    "y_context_np, y_target_np = create_sliding_windows(data, context_points, pred_len)\n",
    "\n",
    "# Convert to torch tensors\n",
    "y_context = torch.tensor(y_context_np, dtype=torch.float32)  # shape: (batch, context_points, 7)\n",
    "y_target = torch.tensor(y_target_np, dtype=torch.float32)    # shape: (batch, pred_len, 7)\n",
    "\n",
    "print(\"y_context shape:\", y_context.shape)\n",
    "print(\"y_target shape:\", y_target.shape)\n",
    "\n",
    "# Create dummy inputs for x_c and x_t (assuming no exogenous features)\n",
    "# If you have exogenous features, replace these with proper tensors.\n",
    "batch = y_context.shape[0]\n",
    "x_context = torch.empty(batch, context_points, 0)  # no features\n",
    "x_target = torch.empty(batch, pred_len, 0)           # no features\n",
    "\n",
    "# d_yt is the number of features (7 in this example)\n",
    "d_yt = len(feature_columns)\n",
    "\n",
    "# Instantiate the forecaster.\n",
    "# Notice that for RevIN to work (if enabled), d_yc must equal d_yt.\n",
    "# Here we disable RevIN by setting use_revin=False.\n",
    "model = Linear_Forecaster(\n",
    "    d_x=0,            # No exogenous input in this example\n",
    "    d_yc=d_yt,\n",
    "    d_yt=d_yt,\n",
    "    context_points=context_points,\n",
    "    use_revin=True,  # set True only if your use case requires and d_yc==d_yt holds\n",
    "    use_seasonal_decomp=False,\n",
    "    linear_window=0\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# A simple training loop\n",
    "def train(model, optimizer, criterion, x_c, y_c, x_t, y_t, epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass returns a tuple, so use the first element\n",
    "        preds = model(x_c, y_c, x_t, y_t)\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        loss = criterion(preds, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "train(model, optimizer, criterion, x_context, y_context, x_target, y_target, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
