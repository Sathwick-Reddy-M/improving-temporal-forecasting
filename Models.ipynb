{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import spacetimeformer as stf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CSVTimeSeries:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str = None,\n",
    "        raw_df: pd.DataFrame = None,\n",
    "        target_cols: List[str] = [],\n",
    "        ignore_cols: List[str] = [],\n",
    "        remove_target_from_context_cols: List[str] = [],\n",
    "        time_col_name: str = \"Datetime\",\n",
    "        read_csv_kwargs={},\n",
    "        val_split: float = 0.15,\n",
    "        test_split: float = 0.15,\n",
    "        normalize: bool = True,\n",
    "        drop_all_nan: bool = False,\n",
    "        time_features: List[str] = [\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"day\",\n",
    "            \"weekday\",\n",
    "            \"hour\",\n",
    "            \"minute\",\n",
    "        ],\n",
    "    ):\n",
    "\n",
    "        assert data_path is not None or raw_df is not None\n",
    "\n",
    "        if raw_df is None:\n",
    "            self.data_path = data_path\n",
    "            assert os.path.exists(self.data_path)\n",
    "            raw_df = pd.read_csv(\n",
    "                self.data_path,\n",
    "                **read_csv_kwargs,\n",
    "            )\n",
    "\n",
    "        if drop_all_nan:\n",
    "            raw_df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "        self.time_col_name = time_col_name\n",
    "        assert self.time_col_name in raw_df.columns\n",
    "\n",
    "        if not target_cols:\n",
    "            target_cols = raw_df.columns.tolist()\n",
    "            target_cols.remove(time_col_name)\n",
    "\n",
    "        if ignore_cols:\n",
    "            if ignore_cols == \"all\":\n",
    "                ignore_cols = raw_df.columns.difference(target_cols).tolist()\n",
    "                ignore_cols.remove(self.time_col_name)\n",
    "            raw_df.drop(columns=ignore_cols, inplace=True)\n",
    "\n",
    "        time_df = pd.to_datetime(raw_df[self.time_col_name], format=\"%Y-%m-%d %H:%M\")\n",
    "        df = stf.data.timefeatures.time_features(\n",
    "            time_df,\n",
    "            raw_df,\n",
    "            time_col_name=self.time_col_name,\n",
    "            use_features=time_features,\n",
    "        )\n",
    "        self.time_cols = df.columns.difference(raw_df.columns)\n",
    "\n",
    "        # Train/Val/Test Split using holdout approach #\n",
    "\n",
    "        def mask_intervals(mask, intervals, cond):\n",
    "            for (interval_low, interval_high) in intervals:\n",
    "                if interval_low is None:\n",
    "                    interval_low = df[self.time_col_name].iloc[0].year\n",
    "                if interval_high is None:\n",
    "                    interval_high = df[self.time_col_name].iloc[-1].year\n",
    "                mask[\n",
    "                    (df[self.time_col_name] >= interval_low)\n",
    "                    & (df[self.time_col_name] <= interval_high)\n",
    "                ] = cond\n",
    "            return mask\n",
    "\n",
    "        test_cutoff = len(time_df) - max(round(test_split * len(time_df)), 1)\n",
    "        val_cutoff = test_cutoff - round(val_split * len(time_df))\n",
    "\n",
    "        val_interval_low = time_df.iloc[val_cutoff]\n",
    "        val_interval_high = time_df.iloc[test_cutoff - 1]\n",
    "        val_intervals = [(val_interval_low, val_interval_high)]\n",
    "\n",
    "        test_interval_low = time_df.iloc[test_cutoff]\n",
    "        test_interval_high = time_df.iloc[-1]\n",
    "        test_intervals = [(test_interval_low, test_interval_high)]\n",
    "\n",
    "        train_mask = df[self.time_col_name] > pd.Timestamp.min\n",
    "        val_mask = df[self.time_col_name] > pd.Timestamp.max\n",
    "        test_mask = df[self.time_col_name] > pd.Timestamp.max\n",
    "        train_mask = mask_intervals(train_mask, test_intervals, False)\n",
    "        train_mask = mask_intervals(train_mask, val_intervals, False)\n",
    "        val_mask = mask_intervals(val_mask, val_intervals, True)\n",
    "        test_mask = mask_intervals(test_mask, test_intervals, True)\n",
    "\n",
    "        if (train_mask == False).all():\n",
    "            print(f\"No training data detected for file {data_path}\")\n",
    "\n",
    "        self._train_data = df[train_mask]\n",
    "        self._scaler = StandardScaler()\n",
    "\n",
    "        self.target_cols = target_cols\n",
    "        for col in remove_target_from_context_cols:\n",
    "            assert (\n",
    "                col in self.target_cols\n",
    "            ), \"`remove_target_from_context_cols` should be target cols that you want to remove from the context\"\n",
    "\n",
    "        self.remove_target_from_context_cols = remove_target_from_context_cols\n",
    "        not_exo_cols = self.time_cols.tolist() + target_cols\n",
    "        self.exo_cols = df.columns.difference(not_exo_cols).tolist()\n",
    "        self.exo_cols.remove(self.time_col_name)\n",
    "\n",
    "        self._train_data = df[train_mask]\n",
    "        self._val_data = df[val_mask]\n",
    "        if test_split == 0.0:\n",
    "            print(\"`test_split` set to 0. Using Val set as Test set.\")\n",
    "            self._test_data = df[val_mask]\n",
    "        else:\n",
    "            self._test_data = df[test_mask]\n",
    "\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self._scaler = self._scaler.fit(\n",
    "                self._train_data[target_cols + self.exo_cols].values\n",
    "            )\n",
    "        self._train_data = self.apply_scaling_df(self._train_data)\n",
    "        self._val_data = self.apply_scaling_df(self._val_data)\n",
    "        self._test_data = self.apply_scaling_df(self._test_data)\n",
    "\n",
    "    def make_hists(self):\n",
    "        for col in self.target_cols + self.exo_cols:\n",
    "            train = self._train_data[col]\n",
    "            test = self._test_data[col]\n",
    "            bins = np.linspace(-5, 5, 80)  # warning: edit bucket limits\n",
    "            plt.hist(train, bins, alpha=0.5, label=\"Train\", density=True)\n",
    "            plt.hist(test, bins, alpha=0.5, label=\"Test\", density=True)\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.title(col)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{col}-hist.png\")\n",
    "            plt.clf()\n",
    "\n",
    "    def get_slice(self, split, start, stop, skip):\n",
    "        assert split in [\"train\", \"val\", \"test\"]\n",
    "        if split == \"train\":\n",
    "            return self.train_data.iloc[start:stop:skip]\n",
    "        elif split == \"val\":\n",
    "            return self.val_data.iloc[start:stop:skip]\n",
    "        else:\n",
    "            return self.test_data.iloc[start:stop:skip]\n",
    "\n",
    "    def apply_scaling(self, array):\n",
    "        if not self.normalize:\n",
    "            return array\n",
    "        dim = array.shape[-1]\n",
    "        return (array - self._scaler.mean_[:dim]) / self._scaler.scale_[:dim]\n",
    "\n",
    "    def apply_scaling_df(self, df):\n",
    "        if not self.normalize:\n",
    "            return df\n",
    "        scaled = df.copy(deep=True)\n",
    "        cols = self.target_cols + self.exo_cols\n",
    "        dtype = df[cols].values.dtype\n",
    "        scaled[cols] = (\n",
    "            df[cols].values - self._scaler.mean_.astype(dtype)\n",
    "        ) / self._scaler.scale_.astype(dtype)\n",
    "        return scaled\n",
    "\n",
    "    def reverse_scaling_df(self, df):\n",
    "        if not self.normalize:\n",
    "            return df\n",
    "        scaled = df.copy(deep=True)\n",
    "        cols = self.target_cols + self.exo_cols\n",
    "        dtype = df[cols].values.dtype\n",
    "        scaled[cols] = (\n",
    "            df[cols].values * self._scaler.scale_.astype(dtype)\n",
    "        ) + self._scaler.mean_.astype(dtype)\n",
    "        return scaled\n",
    "\n",
    "    def reverse_scaling(self, array):\n",
    "        if not self.normalize:\n",
    "            return array\n",
    "        # self._scaler is fit for target_cols + exo_cols\n",
    "        # if the array dim is less than this length we start\n",
    "        # slicing from the target cols\n",
    "        dim = array.shape[-1]\n",
    "        return (array * self._scaler.scale_[:dim]) + self._scaler.mean_[:dim]\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        return self._train_data\n",
    "\n",
    "    @property\n",
    "    def val_data(self):\n",
    "        return self._val_data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        return self._test_data\n",
    "\n",
    "    def length(self, split):\n",
    "        return {\n",
    "            \"train\": len(self.train_data),\n",
    "            \"val\": len(self.val_data),\n",
    "            \"test\": len(self.test_data),\n",
    "        }[split]\n",
    "\n",
    "\n",
    "class CSVTorchDset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_time_series: CSVTimeSeries,\n",
    "        split: str = \"train\",\n",
    "        context_points: int = 128,\n",
    "        target_points: int = 32,\n",
    "        time_resolution: int = 1,\n",
    "    ):\n",
    "        assert split in [\"train\", \"val\", \"test\"]\n",
    "        self.split = split\n",
    "        self.series = csv_time_series\n",
    "        self.context_points = context_points\n",
    "        self.target_points = target_points\n",
    "        self.time_resolution = time_resolution\n",
    "\n",
    "        self._slice_start_points = [\n",
    "            i\n",
    "            for i in range(\n",
    "                0,\n",
    "                self.series.length(split)\n",
    "                + time_resolution * (-target_points - context_points)\n",
    "                + 1,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._slice_start_points)\n",
    "\n",
    "    def _torch(self, *dfs):\n",
    "        return tuple(torch.from_numpy(x.values).float() for x in dfs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = self._slice_start_points[i]\n",
    "        series_slice = self.series.get_slice(\n",
    "            self.split,\n",
    "            start=start,\n",
    "            stop=start\n",
    "            + self.time_resolution * (self.context_points + self.target_points),\n",
    "            skip=self.time_resolution,\n",
    "        )\n",
    "        series_slice = series_slice.drop(columns=[self.series.time_col_name])\n",
    "        ctxt_slice, trgt_slice = (\n",
    "            series_slice.iloc[: self.context_points],\n",
    "            series_slice.iloc[self.context_points :],\n",
    "        )\n",
    "\n",
    "        ctxt_x = ctxt_slice[self.series.time_cols]\n",
    "        trgt_x = trgt_slice[self.series.time_cols]\n",
    "\n",
    "        ctxt_y = ctxt_slice[self.series.target_cols + self.series.exo_cols]\n",
    "        ctxt_y = ctxt_y.drop(columns=self.series.remove_target_from_context_cols)\n",
    "\n",
    "        trgt_y = trgt_slice[self.series.target_cols]\n",
    "\n",
    "        return self._torch(ctxt_x, ctxt_y, trgt_x, trgt_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class DataModule():\n",
    "    def __init__(\n",
    "        self,\n",
    "        datasetCls,\n",
    "        dataset_kwargs: dict,\n",
    "        batch_size: int,\n",
    "        workers: int,\n",
    "        collate_fn=None,\n",
    "        overfit: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.datasetCls = datasetCls\n",
    "        self.batch_size = batch_size\n",
    "        if \"split\" in dataset_kwargs.keys():\n",
    "            del dataset_kwargs[\"split\"]\n",
    "        self.dataset_kwargs = dataset_kwargs\n",
    "        self.workers = workers\n",
    "        self.collate_fn = collate_fn\n",
    "        if overfit:\n",
    "            warnings.warn(\"Overriding val and test dataloaders to use train set!\")\n",
    "        self.overfit = overfit\n",
    "\n",
    "    def train_dataloader(self, shuffle=True):\n",
    "        return self._make_dloader(\"train\", shuffle=shuffle)\n",
    "\n",
    "    def val_dataloader(self, shuffle=False):\n",
    "        return self._make_dloader(\"val\", shuffle=shuffle)\n",
    "\n",
    "    def test_dataloader(self, shuffle=False):\n",
    "        return self._make_dloader(\"test\", shuffle=shuffle)\n",
    "\n",
    "    def _make_dloader(self, split, shuffle=False):\n",
    "        if self.overfit:\n",
    "            split = \"train\"\n",
    "            shuffle = True\n",
    "        return DataLoader(\n",
    "            self.datasetCls(**self.dataset_kwargs, split=split),\n",
    "            shuffle=shuffle,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RevIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reversible Instance Normalization from \n",
    "https://github.com/ts-kim/RevIN\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MovingAvg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeriesDecomposition(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str, update_stats=True):\n",
    "        assert x.ndim == 3\n",
    "        if mode == \"norm\":\n",
    "            if update_stats:\n",
    "                self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == \"denorm\":\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(\n",
    "            torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps\n",
    "        ).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, input_dim=6, embed_dim=512, act_function=torch.sin):\n",
    "        assert embed_dim % input_dim == 0\n",
    "        super(Time2Vec, self).__init__()\n",
    "        self.enabled = embed_dim > 0\n",
    "        if self.enabled:\n",
    "            self.embed_dim = embed_dim // input_dim\n",
    "            self.input_dim = input_dim\n",
    "            self.embed_weight = nn.Parameter(torch.randn(self.input_dim, self.embed_dim))\n",
    "            self.embed_bias = nn.Parameter(torch.randn(self.input_dim, self.embed_dim))\n",
    "            self.act_function = act_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.enabled:\n",
    "            x = torch.diag_embed(x)\n",
    "            # x.shape = (bs, sequence_length, input_dim, input_dim)\n",
    "            x_affine = torch.matmul(x, self.embed_weight) + self.embed_bias\n",
    "            # x_affine.shape = (bs, sequence_length, input_dim, time_embed_dim)\n",
    "            x_affine_0, x_affine_remain = torch.split(\n",
    "                x_affine, [1, self.embed_dim - 1], dim=-1\n",
    "            )\n",
    "            x_affine_remain = self.act_function(x_affine_remain)\n",
    "            x_output = torch.cat([x_affine_0, x_affine_remain], dim=-1)\n",
    "            x_output = x_output.view(x_output.size(0), x_output.size(1), -1)\n",
    "            # x_output.shape = (bs, sequence_length, input_dim * time_embed_dim)\n",
    "        else:\n",
    "            x_output = x\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EPSILON = 1e-7\n",
    "\n",
    "\n",
    "def r_squared(actual: np.ndarray, predicted: np.ndarray):\n",
    "    rss = (_error(actual, predicted) ** 2).sum(1)\n",
    "    tss = (_error(actual, actual.mean(1, keepdims=True)) ** 2).sum(1)\n",
    "    r2 = 1.0 - rss / (tss + EPSILON)\n",
    "    return r2.mean()\n",
    "\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Simple error\"\"\"\n",
    "    return actual - predicted\n",
    "\n",
    "\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "\n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def mape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "    return np.mean(np.abs(_percentage_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def smape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    return np.mean(\n",
    "        2.0\n",
    "        * np.abs(actual - predicted)\n",
    "        / ((np.abs(actual) + np.abs(predicted)) + EPSILON)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TeacherForcingAnnealCallback:\n",
    "    def __init__(self, start: float, end: float, steps: int):\n",
    "        assert start >= end, \"start must be >= end\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.slope = float(start - end) / steps\n",
    "\n",
    "    def on_train_batch_end(self, model: torch.nn.Module):\n",
    "        current = model.teacher_forcing_prob\n",
    "        new_teacher_forcing_prob = max(self.end, current - self.slope)\n",
    "        model.teacher_forcing_prob = new_teacher_forcing_prob\n",
    "        print(\"Teacher Forcing Prob:\", new_teacher_forcing_prob)\n",
    "\n",
    "\n",
    "class TimeMaskedLossCallback:\n",
    "    def __init__(self, start, end, steps):\n",
    "        assert start <= end, \"end must be >= start\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.slope = float(end - start) / steps\n",
    "        self._time_mask = self.start\n",
    "\n",
    "    @property\n",
    "    def time_mask(self):\n",
    "        return round(self._time_mask)\n",
    "\n",
    "    def on_train_start(self, model: torch.nn.Module):\n",
    "        if model.time_masked_idx is None:\n",
    "            model.time_masked_idx = self.time_mask\n",
    "\n",
    "    def on_train_batch_end(self, model: torch.nn.Module):\n",
    "        self._time_mask = min(self.end, self._time_mask + self.slope)\n",
    "        model.time_masked_idx = self.time_mask\n",
    "        print(\"Time Masked Index:\", self.time_mask)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dirpath: str,\n",
    "        monitor: str = \"val/loss\",\n",
    "        mode: str = \"min\",\n",
    "        filename: str = \"\",\n",
    "        save_top_k: int = 1,\n",
    "        auto_insert_metric_name: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dirpath (str): Directory where checkpoints are saved.\n",
    "            monitor (str): The key to monitor (e.g., \"val/loss\").\n",
    "            mode (str): If \"min\", lower monitored values are better; if \"max\", higher are better.\n",
    "            filename (str): A filename prefix. With auto_insert_metric_name true, the metric name is added automatically.\n",
    "            save_top_k (int): How many top models to save. (This simple version only keeps the best model.)\n",
    "            auto_insert_metric_name (bool): Whether to automatically insert the monitored metric name in the filename.\n",
    "        \"\"\"\n",
    "        self.dirpath = dirpath\n",
    "        os.makedirs(self.dirpath, exist_ok=True)\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.filename_prefix = filename\n",
    "        self.save_top_k = save_top_k  # This simple implementation only saves the best model.\n",
    "        self.auto_insert_metric_name = auto_insert_metric_name\n",
    "        \n",
    "        # Internal state: best score seen so far.\n",
    "        self.best_score = None\n",
    "\n",
    "    def _is_improvement(self, current: float) -> bool:\n",
    "        # If no best score yet, consider current as improvement.\n",
    "        if self.best_score is None:\n",
    "            return True\n",
    "        if self.mode == \"min\":\n",
    "            return current < self.best_score\n",
    "        else:\n",
    "            return current > self.best_score\n",
    "\n",
    "    def on_validation_epoch_end(self, epoch: int, metrics: dict, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        To be called at the end of a validation epoch.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): Current epoch number.\n",
    "            metrics (dict): Dictionary that must contain the monitored metric (e.g. {\"val/loss\": value}).\n",
    "            model (torch.nn.Module): The model to be checkpointed.\n",
    "        \"\"\"\n",
    "        current = metrics.get(self.monitor)\n",
    "        if current is None:\n",
    "            print(f\"Metric '{self.monitor}' not found in metrics; skipping checkpoint.\")\n",
    "            return\n",
    "\n",
    "        if self._is_improvement(current):\n",
    "            self.best_score = current\n",
    "            # Build filename: insert metric info if requested\n",
    "            metric_str = f\"_{self.monitor.replace('/', '_')}={current:.4f}\" if self.auto_insert_metric_name else \"\"\n",
    "            filename = f\"{self.filename_prefix}{epoch:02d}{metric_str}.pt\"\n",
    "            filepath = os.path.join(self.dirpath, filename)\n",
    "            torch.save(model.state_dict(), filepath)\n",
    "            print(f\"[Epoch {epoch:02d}] Checkpoint saved to: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from einops import rearrange\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, context_points: int, shared_weights: bool = False, d_yt: int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        if not shared_weights:\n",
    "            assert d_yt is not None\n",
    "            layer_count = d_yt\n",
    "        else:\n",
    "            layer_count = 1\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones((context_points, layer_count)), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones((layer_count)), requires_grad=True)\n",
    "\n",
    "        d = math.sqrt(1.0 / context_points)\n",
    "        self.weights.data.uniform_(-d, d)\n",
    "        self.bias.data.uniform_(-d, d)\n",
    "\n",
    "        self.window = context_points\n",
    "        self.shared_weights = shared_weights\n",
    "        self.d_yt = d_yt\n",
    "\n",
    "    def forward(self, y_c: torch.Tensor, pred_len: int, d_yt: int = None):\n",
    "        batch, length, d_yc = y_c.shape\n",
    "        d_yt = d_yt or self.d_yt\n",
    "\n",
    "        output = torch.zeros(batch, pred_len, d_yt, device=y_c.device)\n",
    "\n",
    "        for i in range(pred_len):\n",
    "            inp = torch.cat((y_c[:, i:, :d_yt], output[:, :i]), dim=1)\n",
    "            output[:, i, :] = self._inner_forward(inp)\n",
    "        return output\n",
    "\n",
    "    def _inner_forward(self, inp):\n",
    "        batch = inp.shape[0]\n",
    "        if self.shared_weights:\n",
    "            inp = rearrange(inp, \"batch length dy -> (batch dy) length 1\")\n",
    "        baseline = (self.weights * inp[:, -self.window :, :]).sum(1) + self.bias\n",
    "        if self.shared_weights:\n",
    "            baseline = rearrange(baseline, \"(batch dy) 1 -> batch dy\", batch=batch)\n",
    "        return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, y_c, target, epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred_len = target.shape[1]\n",
    "        output = model(y_c, pred_len)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETTm1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_sliding_windows(data: np.ndarray, context_points: int, pred_len: int):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (T, d_yt)\n",
    "    returns: y_context (batch, context_points, d_yt) and\n",
    "             y_target (batch, pred_len, d_yt)\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    total_length = data.shape[0]\n",
    "    # Create sliding windows\n",
    "    for i in range(total_length - context_points - pred_len + 1):\n",
    "        X.append(data[i : i + context_points])\n",
    "        Y.append(data[i + context_points : i + context_points + pred_len])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_context shape: torch.Size([69666, 10, 7])\n",
      "y_target shape: torch.Size([69666, 5, 7])\n",
      "Epoch 0, Loss: 70.6213\n",
      "Epoch 100, Loss: 7.1133\n",
      "Epoch 200, Loss: 4.4918\n",
      "Epoch 300, Loss: 4.1479\n",
      "Epoch 400, Loss: 3.7757\n",
      "Epoch 500, Loss: 3.4020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model using all training examples as one batch.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m train(model, optimizer, criterion, y_context, y_target, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, y_c, target, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(y_c, pred_len)\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m----> 8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_path = \"S:\\\\spatiotemporal-analysis\\\\ETTm1_modified.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Sort by date (if not already sorted)\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# Use the numeric columns as features (d_yt should equal the number of features: 7)\n",
    "feature_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "data = df[feature_columns].values  # shape (T, 7)\n",
    "\n",
    "# Define context and prediction lengths\n",
    "context_points = 10\n",
    "pred_len = 5\n",
    "\n",
    "# Create sliding windows from the data\n",
    "y_context_np, y_target_np = create_sliding_windows(data, context_points, pred_len)\n",
    "\n",
    "# Convert to torch tensors\n",
    "y_context = torch.tensor(y_context_np, dtype=torch.float32)\n",
    "y_target = torch.tensor(y_target_np, dtype=torch.float32)\n",
    "\n",
    "print(\"y_context shape:\", y_context.shape)  # (batch, context_points, 7)\n",
    "print(\"y_target shape:\", y_target.shape)    # (batch, pred_len, 7)\n",
    "\n",
    "# Create the model, optimizer, and loss function\n",
    "d_yt = len(feature_columns)\n",
    "model = LinearModel(context_points=context_points, shared_weights=False, d_yt=d_yt)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model using all training examples as one batch.\n",
    "train(model, optimizer, criterion, y_context, y_target, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecaster - The parent class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Forecaster(nn.Module, ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_x: int,\n",
    "        d_yc: int,\n",
    "        d_yt: int,\n",
    "        learning_rate: float = 1e-3,\n",
    "        l2_coeff: float = 0,\n",
    "        loss: str = \"mse\",\n",
    "        linear_window: int = 0,\n",
    "        linear_shared_weights: bool = False,\n",
    "        use_revin: bool = False,\n",
    "        use_seasonal_decomp: bool = False,\n",
    "        verbose: int = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        qprint = lambda msg: print(msg) if verbose else None\n",
    "        qprint(\"Forecaster\")\n",
    "        qprint(f\"\\tL2: {l2_coeff}\")\n",
    "        qprint(f\"\\tLinear Window: {linear_window}\")\n",
    "        qprint(f\"\\tLinear Shared Weights: {linear_shared_weights}\")\n",
    "        qprint(f\"\\tRevIN: {use_revin}\")\n",
    "        qprint(f\"\\tDecomposition: {use_seasonal_decomp}\")\n",
    "\n",
    "        self._inv_scaler = lambda x: x\n",
    "        self.l2_coeff = l2_coeff\n",
    "        self.learning_rate = learning_rate\n",
    "        self.time_masked_idx = None\n",
    "        self.null_value = None\n",
    "        self.loss = loss\n",
    "\n",
    "        if linear_window:\n",
    "            self.linear_model = LinearModel(\n",
    "                linear_window, shared_weights=linear_shared_weights, d_yt=d_yt\n",
    "            )\n",
    "        else:\n",
    "            self.linear_model = lambda x, *args, **kwargs: 0.0\n",
    "\n",
    "        self.use_revin = use_revin\n",
    "        if use_revin:\n",
    "            assert d_yc == d_yt, \"TODO: figure out exo case for revin\"\n",
    "            self.revin = RevIN(num_features=d_yc)\n",
    "        else:\n",
    "            self.revin = lambda x, **kwargs: x\n",
    "\n",
    "        self.use_seasonal_decomp = use_seasonal_decomp\n",
    "        if use_seasonal_decomp:\n",
    "            self.seasonal_decomp = SeriesDecomposition(kernel_size=25)\n",
    "        else:\n",
    "            self.seasonal_decomp = lambda x: (x, x.clone())\n",
    "\n",
    "        self.d_x = d_x\n",
    "        self.d_yc = d_yc\n",
    "        self.d_yt = d_yt\n",
    "\n",
    "    def set_null_value(self, val: float) -> None:\n",
    "        self.null_value = val\n",
    "\n",
    "    def set_inv_scaler(self, scaler) -> None:\n",
    "        self._inv_scaler = scaler\n",
    "\n",
    "    def set_scaler(self, scaler) -> None:\n",
    "        self._scaler = scaler\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def train_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def eval_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    def loss_fn(self, true: torch.Tensor, preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        true = torch.nan_to_num(true)\n",
    "        if self.loss == \"mse\":\n",
    "            loss = (mask * (true - preds)).square().sum() / max(mask.sum(), 1)\n",
    "        elif self.loss == \"mae\":\n",
    "            loss = torch.abs(mask * (true - preds)).sum() / max(mask.sum(), 1)\n",
    "        elif self.loss == \"smape\":\n",
    "            num = 2.0 * torch.abs(preds - true)\n",
    "            den = torch.abs(preds.detach()) + torch.abs(true) + 1e-5\n",
    "            loss = 100.0 * (mask * (num / den)).sum() / max(mask.sum(), 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized Loss Function: {self.loss}\")\n",
    "        return loss\n",
    "\n",
    "    def forecasting_loss(self, outputs: torch.Tensor, y_t: torch.Tensor, time_mask: int) -> Tuple[torch.Tensor]:\n",
    "        if self.null_value is not None:\n",
    "            null_mask_mat = y_t != self.null_value\n",
    "        else:\n",
    "            null_mask_mat = torch.ones_like(y_t)\n",
    "        null_mask_mat *= ~torch.isnan(y_t)\n",
    "\n",
    "        time_mask_mat = torch.ones_like(y_t)\n",
    "        if time_mask is not None:\n",
    "            time_mask_mat[:, time_mask:] = False\n",
    "\n",
    "        full_mask = time_mask_mat * null_mask_mat\n",
    "        forecasting_loss = self.loss_fn(y_t, outputs, full_mask)\n",
    "        return forecasting_loss, full_mask\n",
    "\n",
    "    def compute_loss(self, batch: Tuple[torch.Tensor], time_mask: int = None, forward_kwargs: dict = {}) -> Tuple[torch.Tensor]:\n",
    "        x_c, y_c, x_t, y_t = batch\n",
    "        outputs, *_ = self.forward(x_c, y_c, x_t, y_t, **forward_kwargs)\n",
    "        loss, mask = self.forecasting_loss(outputs=outputs, y_t=y_t, time_mask=time_mask)\n",
    "        return loss, outputs, mask\n",
    "\n",
    "    def predict(self, x_c: torch.Tensor, y_c: torch.Tensor, x_t: torch.Tensor, sample_preds: bool = False) -> torch.Tensor:\n",
    "        og_device = y_c.device\n",
    "        # Ensure tensors are on the same device as the model.\n",
    "        x_c = x_c.to(next(self.parameters()).device).float()\n",
    "        x_t = x_t.to(next(self.parameters()).device).float()\n",
    "        y_c = torch.from_numpy(self._scaler(y_c.cpu().numpy())).to(next(self.parameters()).device).float()\n",
    "        y_t = torch.zeros((x_t.shape[0], x_t.shape[1], self.d_yt), device=next(self.parameters()).device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            normalized_preds, *_ = self.forward(x_c, y_c, x_t, y_t, **self.eval_step_forward_kwargs)\n",
    "        preds = torch.from_numpy(self._inv_scaler(normalized_preds.cpu().numpy())).to(og_device).float()\n",
    "        return preds\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_model_pass(\n",
    "        self,\n",
    "        x_c: torch.Tensor,\n",
    "        y_c: torch.Tensor,\n",
    "        x_t: torch.Tensor,\n",
    "        y_t: torch.Tensor,\n",
    "        **forward_kwargs,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        return NotImplemented\n",
    "\n",
    "    def nan_to_num(self, *inps):\n",
    "        return (torch.nan_to_num(i) for i in inps)\n",
    "\n",
    "    def forward(self, x_c: torch.Tensor, y_c: torch.Tensor, x_t: torch.Tensor, y_t: torch.Tensor, **forward_kwargs) -> Tuple[torch.Tensor]:\n",
    "        x_c, y_c, x_t, y_t = self.nan_to_num(x_c, y_c, x_t, y_t)\n",
    "        _, pred_len, d_yt = y_t.shape\n",
    "\n",
    "        y_c = self.revin(y_c, mode=\"norm\")\n",
    "        seasonal_yc, trend_yc = self.seasonal_decomp(y_c)\n",
    "        preds, *extra = self.forward_model_pass(x_c, seasonal_yc, x_t, y_t, **forward_kwargs)\n",
    "        baseline = self.linear_model(trend_yc, pred_len=pred_len, d_yt=d_yt)\n",
    "        output = self.revin(preds + baseline, mode=\"denorm\")\n",
    "\n",
    "        if extra:\n",
    "            return (output,) + tuple(extra)\n",
    "        return (output,)\n",
    "\n",
    "    def _compute_stats(self, pred: torch.Tensor, true: torch.Tensor, mask: torch.Tensor):\n",
    "        pred = pred * mask\n",
    "        true = torch.nan_to_num(true) * mask\n",
    "\n",
    "        adj = mask.mean().cpu().numpy() + 1e-5\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        true = true.detach().cpu().numpy()\n",
    "        scaled_pred = self._inv_scaler(pred)\n",
    "        scaled_true = self._inv_scaler(true)\n",
    "        stats = {\n",
    "            \"mape\": mape(scaled_true, scaled_pred) / adj,\n",
    "            \"mae\": mae(scaled_true, scaled_pred) / adj,\n",
    "            \"mse\": mse(scaled_true, scaled_pred) / adj,\n",
    "            \"smape\": smape(scaled_true, scaled_pred) / adj,\n",
    "            \"norm_mae\": mae(true, pred) / adj,\n",
    "            \"norm_mse\": mse(true, pred) / adj,\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def step(self, batch: Tuple[torch.Tensor], train: bool = False):\n",
    "        kwargs = self.train_step_forward_kwargs if train else self.eval_step_forward_kwargs\n",
    "        time_mask = self.time_masked_idx if train else None\n",
    "        loss, output, mask = self.compute_loss(batch=batch, time_mask=time_mask, forward_kwargs=kwargs)\n",
    "        *_, y_t = batch\n",
    "        stats = self._compute_stats(output, y_t, mask)\n",
    "        stats[\"loss\"] = loss\n",
    "        return stats\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, train=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        stats = self.step(batch, train=False)\n",
    "        self.current_val_stats = stats\n",
    "        return stats\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, train=False)\n",
    "\n",
    "    def _log_stats(self, section, outs):\n",
    "        for key in outs.keys():\n",
    "            stat = outs[key]\n",
    "            if isinstance(stat, np.ndarray) or isinstance(stat, torch.Tensor):\n",
    "                stat = stat.mean()\n",
    "            self.log(f\"{section}/{key}\", stat, sync_dist=True)\n",
    "\n",
    "    def training_step_end(self, outs):\n",
    "        self._log_stats(\"train\", outs)\n",
    "        return {\"loss\": outs[\"loss\"].mean()}\n",
    "\n",
    "    def validation_step_end(self, outs):\n",
    "        self._log_stats(\"val\", outs)\n",
    "        return outs\n",
    "\n",
    "    def test_step_end(self, outs):\n",
    "        self._log_stats(\"test\", outs)\n",
    "        return {\"loss\": outs[\"loss\"].mean()}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self(*batch, **self.eval_step_forward_kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.learning_rate, weight_decay=self.l2_coeff\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=3,\n",
    "            factor=0.2,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val/loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearAR Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Linear_Forecaster(Forecaster):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_x: int,\n",
    "        d_yc: int,\n",
    "        d_yt: int,\n",
    "        context_points: int,\n",
    "        learning_rate: float = 1e-3,\n",
    "        l2_coeff: float = 0,\n",
    "        loss: str = \"mse\",\n",
    "        linear_window: int = 0,\n",
    "        linear_shared_weights: bool = False,\n",
    "        use_revin: bool = False,\n",
    "        use_seasonal_decomp: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            d_x=d_x,\n",
    "            d_yc=d_yc,\n",
    "            d_yt=d_yt,\n",
    "            l2_coeff=l2_coeff,\n",
    "            learning_rate=learning_rate,\n",
    "            loss=loss,\n",
    "            linear_window=linear_window,\n",
    "            linear_shared_weights=linear_shared_weights,\n",
    "            use_revin=use_revin,\n",
    "            use_seasonal_decomp=use_seasonal_decomp,\n",
    "        )\n",
    "\n",
    "        self.model = LinearModel(\n",
    "            context_points, shared_weights=linear_shared_weights, d_yt=d_yt\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def eval_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def train_step_forward_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    def forward_model_pass(self, x_c, y_c, x_t, y_t):\n",
    "        _, pred_len, d_yt = y_t.shape\n",
    "        output = self.model(y_c, pred_len=pred_len, d_yt=d_yt)\n",
    "        return (output,)\n",
    "\n",
    "    @classmethod\n",
    "    def add_cli(self, parser):\n",
    "        super().add_cli(parser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LinearAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "config = {\n",
    "    \"dset\": \"ettm1\",\n",
    "    \"data_path\": \"S:\\\\spatiotemporal-analysis\\\\ETTm1_modified.csv\",\n",
    "    \"context_points\": 10,\n",
    "    \"target_points\": 10,\n",
    "    \"time_resolution\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"workers\": 1,\n",
    "    \"overfit\": False,\n",
    "    \"model\": \"linear\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"l2_coeff\": 0,\n",
    "    \"loss\": \"mse\",\n",
    "    \"linear_window\": 10,\n",
    "    \"linear_shared_weights\": False,\n",
    "    \"use_revin\": False,\n",
    "    \"use_seasonal_decomp\": False,\n",
    "    \"run_name\": \"linear-ettm1\",\n",
    "}\n",
    "\n",
    "config = SimpleNamespace(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODELS = [\"spacetimeformer\", \"mtgnn\", \"heuristic\", \"lstm\", \"lstnet\", \"linear\", \"s4\"]\n",
    "\n",
    "_DSETS = [\n",
    "    \"asos\",\n",
    "    \"metr-la\",\n",
    "    \"pems-bay\",\n",
    "    \"exchange\",\n",
    "    \"precip\",\n",
    "    \"toy2\",\n",
    "    \"solar_energy\",\n",
    "    \"syn\",\n",
    "    \"mnist\",\n",
    "    \"cifar\",\n",
    "    \"copy\",\n",
    "    \"cont_copy\",\n",
    "    \"m4\",\n",
    "    \"wiki\",\n",
    "    \"ettm1\",\n",
    "    \"weather\",\n",
    "    \"monash\",\n",
    "    \"hangzhou\",\n",
    "    \"traffic\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "\n",
    "def create_dset(config):\n",
    "    INV_SCALER = lambda x: x\n",
    "    SCALER = lambda x: x\n",
    "    NULL_VAL = None\n",
    "    PLOT_VAR_IDXS = None\n",
    "    PLOT_VAR_NAMES = None\n",
    "    PAD_VAL = None\n",
    "\n",
    "    if config.dset == \"ettm1\":\n",
    "        target_cols = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "        dset = CSVTimeSeries(\n",
    "            data_path=config.data_path,\n",
    "            target_cols=target_cols,\n",
    "            ignore_cols=[],\n",
    "            val_split=4.0 / 20,  # from informer\n",
    "            test_split=4.0 / 20,  # from informer\n",
    "            time_col_name=\"date\",\n",
    "            time_features=[\"month\", \"day\", \"weekday\", \"hour\"],\n",
    "        )\n",
    "        DATA_MODULE = DataModule(\n",
    "            datasetCls=CSVTorchDset,\n",
    "            dataset_kwargs={\n",
    "                \"csv_time_series\": dset,\n",
    "                \"context_points\": config.context_points,\n",
    "                \"target_points\": config.target_points,\n",
    "                \"time_resolution\": config.time_resolution,\n",
    "            },\n",
    "            batch_size=config.batch_size,\n",
    "            workers=config.workers,\n",
    "            overfit=config.overfit,\n",
    "        )\n",
    "        INV_SCALER = dset.reverse_scaling\n",
    "        SCALER = dset.apply_scaling\n",
    "        NULL_VAL = None\n",
    "        # PAD_VAL = -32.0\n",
    "        PLOT_VAR_NAMES = target_cols\n",
    "        PLOT_VAR_IDXS = [i for i in range(len(target_cols))]\n",
    "\n",
    "    return (\n",
    "        DATA_MODULE,\n",
    "        INV_SCALER,\n",
    "        SCALER,\n",
    "        NULL_VAL,\n",
    "        PLOT_VAR_IDXS,\n",
    "        PLOT_VAR_NAMES,\n",
    "        PAD_VAL,\n",
    "    )\n",
    "\n",
    "def create_model(config):\n",
    "    x_dim, yc_dim, yt_dim = None, None, None\n",
    "\n",
    "    if config.dset == \"ettm1\":\n",
    "        x_dim = 4\n",
    "        yc_dim = 7\n",
    "        yt_dim = 7\n",
    "\n",
    "    assert x_dim is not None\n",
    "    assert yc_dim is not None\n",
    "    assert yt_dim is not None\n",
    "\n",
    "    if config.model == \"linear\":\n",
    "        forecaster = Linear_Forecaster(\n",
    "            d_x=x_dim,\n",
    "            d_yc=yc_dim,\n",
    "            d_yt=yt_dim,\n",
    "            context_points=config.context_points,\n",
    "            learning_rate=config.learning_rate,\n",
    "            l2_coeff=config.l2_coeff,\n",
    "            loss=config.loss,\n",
    "            linear_window=config.linear_window,\n",
    "            linear_shared_weights=config.linear_shared_weights,\n",
    "            use_revin=config.use_revin,\n",
    "            use_seasonal_decomp=config.use_seasonal_decomp,\n",
    "        )\n",
    "\n",
    "    return forecaster\n",
    "\n",
    "def create_callbacks(config, save_dir):\n",
    "    # Create a unique checkpoint directory (for saving the best model)\n",
    "    filename = f\"{config.run_name}_\" + str(uuid.uuid1()).split(\"-\")[0]\n",
    "    model_ckpt_dir = os.path.join(save_dir, filename)\n",
    "    os.makedirs(model_ckpt_dir, exist_ok=True)\n",
    "    config.model_ckpt_dir = model_ckpt_dir\n",
    "\n",
    "    # Create a list of callback objects.\n",
    "    callbacks = []\n",
    "\n",
    "    # Add custom callbacks if desired (they must have an on_train_batch_end(model) method)\n",
    "    \n",
    "    return callbacks, model_ckpt_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, val_loader, device, epoch, callbacks, criterion=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move each tensor to device. Expecting batch to be a tuple (x_c, y_c, x_t, y_t)\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            # Forward pass. Here we assume your model has a method compute_loss or similar.\n",
    "            loss, outputs, _ = model.compute_loss(batch, time_mask=None, forward_kwargs=model.eval_step_forward_kwargs)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    avg_loss = total_loss / n_batches if n_batches > 0 else float('inf')\n",
    "    \n",
    "    # Call validation callback hooks for checkpointing/early stopping etc.\n",
    "    for cb in callbacks:\n",
    "        if hasattr(cb, \"on_validation_epoch_end\"):\n",
    "            metrics = {\"val/loss\": avg_loss}\n",
    "            cb.on_validation_epoch_end(epoch, metrics, model)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device, args, callbacks):\n",
    "    # Setup for gradient accumulation\n",
    "    accumulate_steps = args.accumulate if hasattr(args, 'accumulate') else 1\n",
    "    gradient_clip = args.grad_clip_norm if hasattr(args, 'grad_clip_norm') else None\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    epoch = 0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    while epoch < args.epochs:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Overfit mode handling: if debug, you might limit the number of batches.\n",
    "\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            loss, outputs, _ = model.compute_loss(batch, time_mask=None, forward_kwargs=model.train_step_forward_kwargs)\n",
    "            loss = loss / accumulate_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Call per-batch callbacks that use on_train_batch_end.\n",
    "            for cb in callbacks:\n",
    "                if hasattr(cb, \"on_train_batch_end\"):\n",
    "                    # We omit trainer parameter for simplicity.\n",
    "                    cb.on_train_batch_end(None, model)\n",
    "            \n",
    "            if (batch_idx + 1) % accumulate_steps == 0:\n",
    "                if gradient_clip is not None:\n",
    "                    # Gradient clipping using norm.\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            running_loss += loss.item() * accumulate_steps  # scale back loss for logging\n",
    "\n",
    "            # Run validation at intervals if desired\n",
    "            if (batch_idx + 1) % val_check_freq == 0:\n",
    "                val_loss = run_validation(model, val_loader, device, epoch, callbacks)\n",
    "                print(f\"[Epoch {epoch:02d}, Batch {batch_idx+1}/{total_train_batches}] Validation Loss: {val_loss:.6f}\")\n",
    "                # Early stopping check if early stopping callback not available\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_epoch = epoch\n",
    "                    early_stop_counter = 0\n",
    "                    # Save checkpoint manually (if no Lightning saving callback is provided)\n",
    "                    ckpt_name = f\"{args.run_name}_{epoch:02d}_val_loss={val_loss:.4f}.pt\"\n",
    "                    ckpt_path = os.path.join(args.model_ckpt_dir, ckpt_name)\n",
    "                    os.makedirs(args.model_ckpt_dir, exist_ok=True)\n",
    "                    torch.save(model.state_dict(), ckpt_path)\n",
    "                    print(f\"Checkpoint saved: {ckpt_path}\")\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= args.patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        return best_epoch, best_val_loss\n",
    "        epoch_loss = running_loss / total_train_batches\n",
    "        print(f\"[Epoch {epoch:02d}] Training Loss: {epoch_loss:.6f}\")\n",
    "        # Run epoch-end validation if not already done inside batch loop\n",
    "        if val_check_freq > total_train_batches:\n",
    "            val_loss = run_validation(model, val_loader, device, epoch, callbacks)\n",
    "            print(f\"[Epoch {epoch:02d}] Validation Loss: {val_loss:.6f}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                early_stop_counter = 0\n",
    "                ckpt_name = f\"{args.run_name}_{epoch:02d}_val_loss={val_loss:.4f}.pt\"\n",
    "                ckpt_path = os.path.join(args.model_ckpt_dir, ckpt_name)\n",
    "                os.makedirs(args.model_ckpt_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                print(f\"Checkpoint saved: {ckpt_path}\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= args.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    return best_epoch, best_val_loss\n",
    "        epoch += 1\n",
    "    return best_epoch, best_val_loss\n",
    "\n",
    "def test(model, test_loader, device, criterion=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            loss, outputs, _ = model.compute_loss(batch, time_mask=None, forward_kwargs=model.eval_step_forward_kwargs)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    avg_loss = total_loss / n_batches if n_batches > 0 else float('inf')\n",
    "    print(f\"Test Loss: {avg_loss:.6f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default log dir: ./data/STF_LOG_DIR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\spatiotemporal-analysis\\spacetimeformer-main\\spacetimeformer\\data\\timefeatures.py:25: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  main_df[\"Month\"] = dates.apply(\n",
      "s:\\spatiotemporal-analysis\\spacetimeformer-main\\spacetimeformer\\data\\timefeatures.py:29: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  main_df[\"Day\"] = dates.apply(lambda row: 2.0 * ((row.day - 1) / 30.0) - 1.0, 1)\n",
      "s:\\spatiotemporal-analysis\\spacetimeformer-main\\spacetimeformer\\data\\timefeatures.py:31: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  main_df[\"Weekday\"] = dates.apply(\n",
      "s:\\spatiotemporal-analysis\\spacetimeformer-main\\spacetimeformer\\data\\timefeatures.py:35: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  main_df[\"Hour\"] = dates.apply(lambda row: 2.0 * ((row.hour) / 23.0) - 1.0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecaster\n",
      "\tL2: 0\n",
      "\tLinear Window: 10\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: False\n",
      "\tDecomposition: False\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    # Setup log directory.\n",
    "    log_dir = os.getenv(\"STF_LOG_DIR\")\n",
    "    if log_dir is None:\n",
    "        log_dir = \"./data/STF_LOG_DIR\"\n",
    "        print(\"Using default log dir: ./data/STF_LOG_DIR\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    # Optionally initialize wandb\n",
    "    logger = None\n",
    "\n",
    "    # Create dataset/dataloaders.\n",
    "    (data_module, inv_scaler, scaler, null_val, plot_var_idxs, plot_var_names, pad_val) = create_dset(args)\n",
    "    \n",
    "    # Create model.\n",
    "    args.null_value = null_val\n",
    "    args.pad_value = pad_val\n",
    "    model = create_model(args)\n",
    "    model.set_inv_scaler(inv_scaler)\n",
    "    model.set_scaler(scaler)\n",
    "    model.set_null_value(null_val)\n",
    "    \n",
    "    # Create callbacks and checkpoint directory.\n",
    "    filename = f\"{args.run_name}_\" + str(uuid.uuid1()).split(\"-\")[0]\n",
    "    model_ckpt_dir = os.path.join(log_dir, filename)\n",
    "    args.model_ckpt_dir = model_ckpt_dir\n",
    "    os.makedirs(model_ckpt_dir, exist_ok=True)\n",
    "    callbacks = create_callbacks(args, save_dir=log_dir)\n",
    "    \n",
    "    \n",
    "    # Decide validation control frequency.\n",
    "    # If args.val_check_interval <= 1.0, it is a fraction, otherwise, epoch interval.\n",
    "    # Here we assume args.epochs exists.\n",
    "    args.epochs = args.epochs if hasattr(args, 'epochs') else 50\n",
    "    \n",
    "    # Use GPU if available.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create dataloaders.\n",
    "    train_loader = data_module.train_dataloader(shuffle=True)\n",
    "    val_loader = data_module.val_dataloader(shuffle=False)\n",
    "    test_loader = data_module.test_dataloader(shuffle=False)\n",
    "    \n",
    "    # Setup optimizer. Adjust LR as needed.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    best_epoch, best_val_loss = train(model, optimizer, train_loader, val_loader, device, args, callbacks)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Training completed in {duration:.1f} seconds; Best epoch: {best_epoch}, Best val loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # Load best checkpoint for testing.\n",
    "    # This sample assumes a checkpoint file naming convention, and you may load manually.\n",
    "    # For simplicity, we assume the best ckpt is the one saved at best_epoch.\n",
    "    ckpt_name = f\"{args.run_name}_{best_epoch:02d}_val_loss={best_val_loss:.4f}.pt\"\n",
    "    ckpt_path = os.path.join(args.model_ckpt_dir, ckpt_name)\n",
    "    print(f\"Loading best checkpoint: {ckpt_path}\")\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    \n",
    "    # Run test.\n",
    "    test_loss = test(model, test_loader, device)\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "config.args = 50\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    (\n",
    "        data_module,\n",
    "        inv_scaler,\n",
    "        scaler,\n",
    "        null_val,\n",
    "        plot_var_idxs,\n",
    "        plot_var_names,\n",
    "        pad_val,\n",
    "    ) = create_dset(config)\n",
    "\n",
    "    # Model\n",
    "    config[\"null_val\"] = null_val\n",
    "    config[\"pad_val\"] = pad_val\n",
    "\n",
    "    forecaster = create_model(config)\n",
    "    forecaster.set_inv_scaler(inv_scaler)\n",
    "    forecaster.set_scaler(scaler)\n",
    "    forecaster.set_null_value(null_val)\n",
    "\n",
    "    callbacks = [] # Need to implement\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While Training should consider the on_train_start and on_train_batch_end methods for the callbacks too\n",
    "# Skipped the callbacks functionality -> need to implement it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETTm1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_sliding_windows(data: np.ndarray, context_points: int, pred_len: int):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (T, d_yt)\n",
    "    returns: y_context (batch, context_points, d_yt) and\n",
    "             y_target (batch, pred_len, d_yt)\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    total_length = data.shape[0]\n",
    "    # Create sliding windows\n",
    "    for i in range(total_length - context_points - pred_len + 1):\n",
    "        X.append(data[i : i + context_points])\n",
    "        Y.append(data[i + context_points : i + context_points + pred_len])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_context shape: torch.Size([69666, 10, 7])\n",
      "y_target shape: torch.Size([69666, 5, 7])\n",
      "Forecaster\n",
      "\tL2: 0\n",
      "\tLinear Window: 0\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: True\n",
      "\tDecomposition: True\n",
      "Epoch 100, Loss: 4.400672\n",
      "Epoch 200, Loss: 3.183222\n",
      "Epoch 300, Loss: 2.524292\n",
      "Epoch 400, Loss: 2.418489\n",
      "Epoch 500, Loss: 2.339239\n",
      "Epoch 600, Loss: 2.266816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m train(model, optimizer, criterion, x_context, y_context, x_target, y_target, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, x_c, y_c, x_t, y_t, epochs)\u001b[0m\n\u001b[0;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Forward pass returns a tuple, so use the first element\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(x_c, y_c, x_t, y_t)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     64\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 152\u001b[0m, in \u001b[0;36mForecaster.forward\u001b[1;34m(self, x_c, y_c, x_t, y_t, **forward_kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m y_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevin(y_c, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m seasonal_yc, trend_yc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseasonal_decomp(y_c)\n\u001b[1;32m--> 152\u001b[0m preds, \u001b[38;5;241m*\u001b[39mextra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_model_pass(x_c, seasonal_yc, x_t, y_t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_kwargs)\n\u001b[0;32m    153\u001b[0m baseline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_model(trend_yc, pred_len\u001b[38;5;241m=\u001b[39mpred_len, d_yt\u001b[38;5;241m=\u001b[39md_yt)\n\u001b[0;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevin(preds \u001b[38;5;241m+\u001b[39m baseline, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdenorm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m, in \u001b[0;36mLinear_Forecaster.forward_model_pass\u001b[1;34m(self, x_c, y_c, x_t, y_t)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_model_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_c, y_c, x_t, y_t):\n\u001b[0;32m     46\u001b[0m     _, pred_len, d_yt \u001b[38;5;241m=\u001b[39m y_t\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 47\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(y_c, pred_len\u001b[38;5;241m=\u001b[39mpred_len, d_yt\u001b[38;5;241m=\u001b[39md_yt)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (output,)\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vm-user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mLinearModel.forward\u001b[1;34m(self, y_c, pred_len, d_yt)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pred_len):\n\u001b[0;32m     29\u001b[0m     inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((y_c[:, i:, :d_yt], output[:, :i]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     output[:, i, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_forward(inp)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mLinearModel._inner_forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_weights:\n\u001b[0;32m     36\u001b[0m     inp \u001b[38;5;241m=\u001b[39m rearrange(inp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length dy -> (batch dy) length 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m baseline \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m*\u001b[39m inp[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow :, :])\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_weights:\n\u001b[0;32m     39\u001b[0m     baseline \u001b[38;5;241m=\u001b[39m rearrange(baseline, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(batch dy) 1 -> batch dy\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Read CSV and prepare data\n",
    "csv_path = \"S:\\\\spatiotemporal-analysis\\\\ETTm1_modified.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Sort by date (if not already sorted)\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# Use the numeric columns as features (d_yt should equal the number of features: 7)\n",
    "feature_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "data = df[feature_columns].values  # shape (T, 7)\n",
    "\n",
    "# Define context and prediction lengths\n",
    "context_points = 10\n",
    "pred_len = 5\n",
    "\n",
    "# Create sliding windows from the data\n",
    "y_context_np, y_target_np = create_sliding_windows(data, context_points, pred_len)\n",
    "\n",
    "# Convert to torch tensors\n",
    "y_context = torch.tensor(y_context_np, dtype=torch.float32)  # shape: (batch, context_points, 7)\n",
    "y_target = torch.tensor(y_target_np, dtype=torch.float32)    # shape: (batch, pred_len, 7)\n",
    "\n",
    "print(\"y_context shape:\", y_context.shape)\n",
    "print(\"y_target shape:\", y_target.shape)\n",
    "\n",
    "# Create dummy inputs for x_c and x_t (assuming no exogenous features)\n",
    "# If you have exogenous features, replace these with proper tensors.\n",
    "batch = y_context.shape[0]\n",
    "x_context = torch.empty(batch, context_points, 0)  # no features\n",
    "x_target = torch.empty(batch, pred_len, 0)           # no features\n",
    "\n",
    "# d_yt is the number of features (7 in this example)\n",
    "d_yt = len(feature_columns)\n",
    "\n",
    "# Instantiate the forecaster.\n",
    "# Notice that for RevIN to work (if enabled), d_yc must equal d_yt.\n",
    "# Here we disable RevIN by setting use_revin=False.\n",
    "model = Linear_Forecaster(\n",
    "    d_x=0,            # No exogenous input in this example\n",
    "    d_yc=d_yt,\n",
    "    d_yt=d_yt,\n",
    "    context_points=context_points,\n",
    "    use_revin=True,  # set True only if your use case requires and d_yc==d_yt holds\n",
    "    use_seasonal_decomp=True,\n",
    "    linear_window=0\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# A simple training loop\n",
    "def train(model, optimizer, criterion, x_c, y_c, x_t, y_t, epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass returns a tuple, so use the first element\n",
    "        preds = model(x_c, y_c, x_t, y_t)\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        loss = criterion(preds, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "train(model, optimizer, criterion, x_context, y_context, x_target, y_target, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
